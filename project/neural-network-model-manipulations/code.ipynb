{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dotha\\AppData\\Local\\Temp\\ipykernel_28560\\3699265438.py:125: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  outputs[int(labels[training_index])-1] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 (learning rate 2, iteration 60), cost: 7.09596155240651\n",
      "Iteration 2 (learning rate 2, iteration 60), cost: 5.7448484814036584\n",
      "Iteration 3 (learning rate 2, iteration 60), cost: 4.11453410280061\n",
      "Iteration 4 (learning rate 2, iteration 60), cost: 3.28249269655821\n",
      "Iteration 5 (learning rate 2, iteration 60), cost: 3.264530851478859\n",
      "Iteration 6 (learning rate 2, iteration 60), cost: 3.2571134638888406\n",
      "Iteration 7 (learning rate 2, iteration 60), cost: 3.2504783952970935\n",
      "Iteration 8 (learning rate 2, iteration 60), cost: 3.2438904078036748\n",
      "Iteration 9 (learning rate 2, iteration 60), cost: 3.2370332338489174\n",
      "Iteration 10 (learning rate 2, iteration 60), cost: 3.2296931248924285\n",
      "Iteration 11 (learning rate 2, iteration 60), cost: 3.2216895191584958\n",
      "Iteration 12 (learning rate 2, iteration 60), cost: 3.212859824079232\n",
      "Iteration 13 (learning rate 2, iteration 60), cost: 3.203055217563154\n",
      "Iteration 14 (learning rate 2, iteration 60), cost: 3.1921314192145958\n",
      "Iteration 15 (learning rate 2, iteration 60), cost: 3.1799298728221923\n",
      "Iteration 16 (learning rate 2, iteration 60), cost: 3.1662519599046863\n",
      "Iteration 17 (learning rate 2, iteration 60), cost: 3.150831909692711\n",
      "Iteration 18 (learning rate 2, iteration 60), cost: 3.1333159401889192\n",
      "Iteration 19 (learning rate 2, iteration 60), cost: 3.113262571302449\n",
      "Iteration 20 (learning rate 2, iteration 60), cost: 3.090194725110602\n",
      "Iteration 21 (learning rate 2, iteration 60), cost: 3.0637359755648146\n",
      "Iteration 22 (learning rate 2, iteration 60), cost: 3.033777275028777\n",
      "Iteration 23 (learning rate 2, iteration 60), cost: 3.0004618509113454\n",
      "Iteration 24 (learning rate 2, iteration 60), cost: 2.9639371453857617\n",
      "Iteration 25 (learning rate 2, iteration 60), cost: 2.9242861759391485\n",
      "Iteration 26 (learning rate 2, iteration 60), cost: 2.88180231430338\n",
      "Iteration 27 (learning rate 2, iteration 60), cost: 2.8371523206584643\n",
      "Iteration 28 (learning rate 2, iteration 60), cost: 2.7911711082037565\n",
      "Iteration 29 (learning rate 2, iteration 60), cost: 2.7445879271897993\n",
      "Iteration 30 (learning rate 2, iteration 60), cost: 2.6979479567818707\n",
      "Iteration 31 (learning rate 2, iteration 60), cost: 2.6516333321736703\n",
      "Iteration 32 (learning rate 2, iteration 60), cost: 2.605866639380148\n",
      "Iteration 33 (learning rate 2, iteration 60), cost: 2.5607163315276265\n",
      "Iteration 34 (learning rate 2, iteration 60), cost: 2.5161610642714036\n",
      "Iteration 35 (learning rate 2, iteration 60), cost: 2.4722265883104235\n",
      "Iteration 36 (learning rate 2, iteration 60), cost: 2.4290627951839387\n",
      "Iteration 37 (learning rate 2, iteration 60), cost: 2.3867757282305813\n",
      "Iteration 38 (learning rate 2, iteration 60), cost: 2.345221036988627\n",
      "Iteration 39 (learning rate 2, iteration 60), cost: 2.3041140992068194\n",
      "Iteration 40 (learning rate 2, iteration 60), cost: 2.263288980028925\n",
      "Iteration 41 (learning rate 2, iteration 60), cost: 2.2227888574871644\n",
      "Iteration 42 (learning rate 2, iteration 60), cost: 2.1827946734177925\n",
      "Iteration 43 (learning rate 2, iteration 60), cost: 2.143579594643532\n",
      "Iteration 44 (learning rate 2, iteration 60), cost: 2.1054365642387847\n",
      "Iteration 45 (learning rate 2, iteration 60), cost: 2.0685204739886283\n",
      "Iteration 46 (learning rate 2, iteration 60), cost: 2.032814918466605\n",
      "Iteration 47 (learning rate 2, iteration 60), cost: 1.9982458260880571\n",
      "Iteration 48 (learning rate 2, iteration 60), cost: 1.9647788536754736\n",
      "Iteration 49 (learning rate 2, iteration 60), cost: 1.9324384813755595\n",
      "Iteration 50 (learning rate 2, iteration 60), cost: 1.9012724552798532\n",
      "Iteration 51 (learning rate 2, iteration 60), cost: 1.871306258163357\n",
      "Iteration 52 (learning rate 2, iteration 60), cost: 1.8425201862323362\n",
      "Iteration 53 (learning rate 2, iteration 60), cost: 1.8148525523607302\n",
      "Iteration 54 (learning rate 2, iteration 60), cost: 1.7882156696283427\n",
      "Iteration 55 (learning rate 2, iteration 60), cost: 1.7625128758833493\n",
      "Iteration 56 (learning rate 2, iteration 60), cost: 1.7376516531726092\n",
      "Iteration 57 (learning rate 2, iteration 60), cost: 1.7135519974535605\n",
      "Iteration 58 (learning rate 2, iteration 60), cost: 1.6901502878458363\n",
      "Iteration 59 (learning rate 2, iteration 60), cost: 1.6673988091215348\n",
      "Iteration 60 (learning rate 2, iteration 60), cost: 1.6452614846229359\n",
      "Thời gian huấn luyện: 32.01 giây\n",
      "Test precision: 0.762\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Structure of the 3-layer neural network.\n",
    "Input_layer_size = 400\n",
    "Hidden_layer_size = 25\n",
    "Output_layer_size = 10\n",
    "\n",
    "\n",
    "def convert_memory_ordering_f2c(array):\n",
    "    if np.isfortran(array) is True:\n",
    "        return np.ascontiguousarray(array)\n",
    "    else:\n",
    "        return array\n",
    "\n",
    "\n",
    "def load_training_data(training_file='mnistdata.mat'):\n",
    "    '''Load training data (mnistdata.mat) and return (inputs, labels).\n",
    "\n",
    "    inputs: numpy array with size (5000, 400).\n",
    "    labels: numpy array with size (5000, 1).\n",
    "\n",
    "    The training data is from Andrew Ng's exercise of the Coursera\n",
    "    machine learning course (ex4data1.mat).\n",
    "    '''\n",
    "    training_data = sio.loadmat(training_file)\n",
    "    inputs = training_data['X'].astype('f8')\n",
    "    inputs = convert_memory_ordering_f2c(inputs)\n",
    "    labels = training_data['y']\n",
    "    labels = convert_memory_ordering_f2c(labels)\n",
    "    return (inputs, labels)\n",
    "\n",
    "\n",
    "def load_weights(weight_file='mnistweights.mat'):\n",
    "    '''Load training data (mnistdata.mat) and return (inputs, labels).\n",
    "\n",
    "    The weights file is from Andrew Ng's exercise of the Coursera\n",
    "    machine learning course (ex4weights.mat).\n",
    "    '''\n",
    "    weights = sio.loadmat(weight_file)\n",
    "    theta1 = convert_memory_ordering_f2c(weights['Theta1'].astype('f8'))  # size: 25 entries, each has 401 numbers\n",
    "    theta2 = convert_memory_ordering_f2c(weights['Theta2'].astype('f8'))  # size: 10 entries, each has  26 numbers\n",
    "    return (theta1, theta2)\n",
    "\n",
    "\n",
    "def rand_init_weights(size_in, size_out):\n",
    "    epsilon_init = 0.12\n",
    "    return np.random.rand(size_out, 1 + size_in) * 2 * epsilon_init - epsilon_init\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + pow(math.e, -z))\n",
    "\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "\n",
    "def cost_function(theta1, theta2, input_layer_size, hidden_layer_size, output_layer_size, inputs, labels, regular=0):\n",
    "    '''\n",
    "    Note: theta1, theta2, inputs, labels are numpy arrays:\n",
    "\n",
    "        theta1: (25, 401)\n",
    "        theta2: (10, 26)\n",
    "        inputs: (5000, 400)\n",
    "        labels: (5000, 1)\n",
    "    '''\n",
    "    # construct neural network\n",
    "    input_layer = np.insert(inputs, 0, 1, axis=1)  # add bias, 5000x401\n",
    "\n",
    "    hidden_layer = np.dot(input_layer, np.transpose(theta1))\n",
    "    hidden_layer = sigmoid(hidden_layer)\n",
    "    hidden_layer = np.insert(hidden_layer, 0, 1, axis=1)  # add bias, 5000x26\n",
    "\n",
    "    output_layer = np.dot(hidden_layer, np.transpose(theta2))  # 5000x10\n",
    "    output_layer = sigmoid(output_layer)\n",
    "    #print('input  layer shape: {}'.format(input_layer.shape))\n",
    "    #print('hidden layer shape: {}'.format(hidden_layer.shape))\n",
    "    #print('output layer shape: {}'.format(output_layer.shape))\n",
    "\n",
    "    # forward propagation: calculate cost\n",
    "    cost = 0.0\n",
    "    for training_index in range(len(inputs)):\n",
    "        # transform label y[i] from a number to a vector.\n",
    "        #\n",
    "        # Note:\n",
    "        #   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        #    1  2  3  4  5  6  7  8  9 10\n",
    "        #\n",
    "        #   if y[i] is 0 -> [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "        #   if y[i] is 1 -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        outputs = [0] * output_layer_size\n",
    "        outputs[int(labels[training_index])-1] = 1\n",
    "\n",
    "        for k in range(output_layer_size):\n",
    "            cost += -outputs[k] * math.log(output_layer[training_index][k]) - (1 - outputs[k]) * math.log(1 - output_layer[training_index][k])\n",
    "    cost /= len(inputs)\n",
    "\n",
    "    # back propagation: calculate gradiants\n",
    "    theta1_grad = np.zeros_like(theta1)  # 25x401\n",
    "    theta2_grad = np.zeros_like(theta2)  # 10x26\n",
    "    for index in range(len(inputs)):\n",
    "        # transform label y[i] from a number to a vector.\n",
    "        outputs = np.zeros((1, output_layer_size))  # (1,10)\n",
    "        outputs[0][labels[index]-1] = 1\n",
    "\n",
    "        # calculate delta3\n",
    "        delta3 = (output_layer[index] - outputs).T  # (10,1)\n",
    "\n",
    "        # calculate delta2\n",
    "        z2 = np.dot(theta1, input_layer[index:index+1].T)  # (25,401) x (401,1)\n",
    "        z2 = np.insert(z2, 0, 1, axis=0)  # add bias, (26,1)\n",
    "        delta2 = np.multiply(\n",
    "            np.dot(theta2.T, delta3),  # (26,10) x (10,1)\n",
    "            sigmoid_gradient(z2)       # (26,1)\n",
    "        )\n",
    "        delta2 = delta2[1:]  # (25,1)\n",
    "\n",
    "        # calculate gradients of theta1 and theta2\n",
    "        # (25,401) = (25,1) x (1,401)\n",
    "        theta1_grad += np.dot(delta2, input_layer[index:index+1])\n",
    "        # (10,26) = (10,1) x (1,26)\n",
    "        theta2_grad += np.dot(delta3, hidden_layer[index:index+1])\n",
    "    theta1_grad /= len(inputs)\n",
    "    theta2_grad /= len(inputs)\n",
    "\n",
    "    return cost, (theta1_grad, theta2_grad)\n",
    "\n",
    "\n",
    "def gradient_descent(inputs, labels, learningrate=0.8, iteration=50):\n",
    "    '''\n",
    "    @return cost and trained model (weights).\n",
    "    '''\n",
    "    rand_theta1 = rand_init_weights(Input_layer_size, Hidden_layer_size)\n",
    "    rand_theta2 = rand_init_weights(Hidden_layer_size, Output_layer_size)\n",
    "    theta1 = rand_theta1\n",
    "    theta2 = rand_theta2\n",
    "    cost = 0.0\n",
    "    for i in range(iteration):\n",
    "        cost, (theta1_grad, theta2_grad) = cost_function(theta1, theta2,\n",
    "            Input_layer_size, Hidden_layer_size, Output_layer_size,\n",
    "            inputs, labels, regular=0)\n",
    "        theta1 -= learningrate * theta1_grad\n",
    "        theta2 -= learningrate * theta2_grad\n",
    "        print('Iteration {0} (learning rate {2}, iteration {3}), cost: {1}'.format(i+1, cost, learningrate, iteration))\n",
    "    return cost, (theta1, theta2)\n",
    "\n",
    "\n",
    "def train(inputs, labels, learningrate=0.8, iteration=50):\n",
    "    cost, model = gradient_descent(inputs, labels, learningrate, iteration)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, inputs):\n",
    "    theta1, theta2 = model\n",
    "    a1 = np.insert(inputs, 0, 1, axis=1)  # add bias, (5000,401)\n",
    "    a2 = np.dot(a1, theta1.T)  # (5000,401) x (401,25)\n",
    "    a2 = sigmoid(a2)\n",
    "    a2 = np.insert(a2, 0, 1, axis=1)  # add bias, (5000,26)\n",
    "    a3 = np.dot(a2, theta2.T)  # (5000,26) x (26,10)\n",
    "    a3 = sigmoid(a3)  # (5000,10)\n",
    "    return [i.argmax()+1 for i in a3]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: There are 10 units which present the digits [1-9, 0]\n",
    "    # (in order) in the output layer.\n",
    "    inputs, labels = load_training_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
    "    # model = train(X_train, y_train, learningrate=2, iteration=60)\n",
    "    \n",
    "    # Đo thời gian bắt đầu huấn luyện\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Huấn luyện mô hình\n",
    "    model = train(X_train, y_train, learningrate=2, iteration=60)\n",
    "\n",
    "    # Đo thời gian kết thúc huấn luyện\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Tính thời gian huấn luyện\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Thời gian huấn luyện: {training_time:.2f} giây\")\n",
    "\n",
    "    # train the model from scratch and predict based on it\n",
    "    # learning rate 0.10, iteration  60: 36% (cost: 3.217)\n",
    "    # learning rate 1.75, iteration  50: 77%\n",
    "    # learning rate 1.90, iteration  50: 75%\n",
    "    # learning rate 2.00, iteration  50: 82%\n",
    "    # learning rate 2.00, iteration 100: 87%\n",
    "    # learning rate 2.00, iteration 200: 93% (cost: 0.572)\n",
    "    # learning rate 2.00, iteration 300: 94% (cost: 0.485)\n",
    "    # learning rate 2.05, iteration  50: 79%\n",
    "    # learning rate 2.20, iteration  50: 64%\n",
    "    # model = train(inputs, labels, learningrate=2, iteration=60)\n",
    "\n",
    "    # Load pretrained weights for debugging precision.\n",
    "    # The precision will be around 97% (0.9756).\n",
    "    #weights = load_weights()\n",
    "    #theta1 = weights[0]  # size: 25 entries, each has 401 numbers\n",
    "    #theta2 = weights[1]  # size: 10 entries, each has  26 numbersf\n",
    "    #model = (theta1, theta2)\n",
    "    #cost, (theta1_grad, theta2_grad) = cost_function(theta1, theta2, Input_layer_size, Hidden_layer_size, Output_layer_size, inputs, labels, regular=0)\n",
    "    #print('cost:', cost)\n",
    "\n",
    "    # outputs = predict(model, inputs)\n",
    "    outputs = predict(model, X_test)\n",
    "    correct_prediction = sum(outputs[i] == y_test[i][0] for i in range(len(y_test)))\n",
    "    precision = correct_prediction / len(y_test)\n",
    "\n",
    "    print('Test precision: {}'.format(precision))\n",
    "\n",
    "    # correct_prediction = 0\n",
    "    # for i, predict in enumerate(outputs):\n",
    "    #     if predict == labels[i][0]:\n",
    "    #         correct_prediction += 1\n",
    "    # precision = float(correct_prediction) / len(labels)\n",
    "    # print('precision: {}'.format(precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU mode\n",
      "NumPy function creation costs 0.025166988372802734 secs\n",
      "\tconstruction: hidden layer dot costs 0.007000923156738281 secs\n",
      "\tconstruction: output layer dot costs 0.002003908157348633 secs\n",
      "\tforward prop: costs 0.07242369651794434 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dotha\\AppData\\Local\\Temp\\ipykernel_28560\\1449145469.py:109: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  outputs[int(labels[training_index])-1] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tback prop: costs 0.9335758686065674 secs\n",
      "cost: 0.2876291651613203\n",
      "\tconstruction: hidden layer dot costs 0.005045175552368164 secs\n",
      "\tconstruction: output layer dot costs 0.0014371871948242188 secs\n",
      "\tforward prop: costs 0.04389476776123047 secs\n",
      "\tback prop: costs 0.533027172088623 secs\n",
      "Iteration 1 (learning rate 2, iteration 60), cost: 7.086078220576518, time: 0.5880126953125\n",
      "\tconstruction: hidden layer dot costs 0.003339529037475586 secs\n",
      "\tconstruction: output layer dot costs 0.0020093917846679688 secs\n",
      "\tforward prop: costs 0.04181408882141113 secs\n",
      "\tback prop: costs 0.5019538402557373 secs\n",
      "Iteration 2 (learning rate 2, iteration 60), cost: 5.742453675713526, time: 0.5532770156860352\n",
      "\tconstruction: hidden layer dot costs 0.004005908966064453 secs\n",
      "\tconstruction: output layer dot costs 0.0010006427764892578 secs\n",
      "\tforward prop: costs 0.04009580612182617 secs\n",
      "\tback prop: costs 0.5059905052185059 secs\n",
      "Iteration 3 (learning rate 2, iteration 60), cost: 3.676355428184077, time: 0.5525381565093994\n",
      "\tconstruction: hidden layer dot costs 0.0046541690826416016 secs\n",
      "\tconstruction: output layer dot costs 0.0 secs\n",
      "\tforward prop: costs 0.04053926467895508 secs\n",
      "\tback prop: costs 0.5117390155792236 secs\n",
      "Iteration 4 (learning rate 2, iteration 60), cost: 3.3091554013684115, time: 0.560936450958252\n",
      "\tconstruction: hidden layer dot costs 0.005005359649658203 secs\n",
      "\tconstruction: output layer dot costs 0.0 secs\n",
      "\tforward prop: costs 0.041184186935424805 secs\n",
      "\tback prop: costs 0.5089914798736572 secs\n",
      "Iteration 5 (learning rate 2, iteration 60), cost: 3.2605579864504888, time: 0.5571765899658203\n",
      "\tconstruction: hidden layer dot costs 0.004431009292602539 secs\n",
      "\tconstruction: output layer dot costs 0.0020427703857421875 secs\n",
      "\tforward prop: costs 0.06086421012878418 secs\n",
      "\tback prop: costs 0.49923062324523926 secs\n",
      "Iteration 6 (learning rate 2, iteration 60), cost: 3.24780426937489, time: 0.5705978870391846\n",
      "\tconstruction: hidden layer dot costs 0.004172325134277344 secs\n",
      "\tconstruction: output layer dot costs 0.001009225845336914 secs\n",
      "\tforward prop: costs 0.043128252029418945 secs\n",
      "\tback prop: costs 0.5243551731109619 secs\n",
      "Iteration 7 (learning rate 2, iteration 60), cost: 3.239661964061191, time: 0.5773186683654785\n",
      "\tconstruction: hidden layer dot costs 0.003461599349975586 secs\n",
      "\tconstruction: output layer dot costs 0.002016782760620117 secs\n",
      "\tforward prop: costs 0.05628037452697754 secs\n",
      "\tback prop: costs 0.5470693111419678 secs\n",
      "Iteration 8 (learning rate 2, iteration 60), cost: 3.2310306121385386, time: 0.6137566566467285\n",
      "\tconstruction: hidden layer dot costs 0.0030367374420166016 secs\n",
      "\tconstruction: output layer dot costs 0.001001596450805664 secs\n",
      "\tforward prop: costs 0.04046344757080078 secs\n",
      "\tback prop: costs 0.5256922245025635 secs\n",
      "Iteration 9 (learning rate 2, iteration 60), cost: 3.2210804527075747, time: 0.5743863582611084\n",
      "\tconstruction: hidden layer dot costs 0.004197597503662109 secs\n",
      "\tconstruction: output layer dot costs 0.0010089874267578125 secs\n",
      "\tforward prop: costs 0.0405573844909668 secs\n",
      "\tback prop: costs 0.5207231044769287 secs\n",
      "Iteration 10 (learning rate 2, iteration 60), cost: 3.2092225697354, time: 0.5679178237915039\n",
      "\tconstruction: hidden layer dot costs 0.004010677337646484 secs\n",
      "\tconstruction: output layer dot costs 0.001003265380859375 secs\n",
      "\tforward prop: costs 0.04018664360046387 secs\n",
      "\tback prop: costs 0.4950718879699707 secs\n",
      "Iteration 11 (learning rate 2, iteration 60), cost: 3.194822076393607, time: 0.5426015853881836\n",
      "\tconstruction: hidden layer dot costs 0.00399470329284668 secs\n",
      "\tconstruction: output layer dot costs 0.0014328956604003906 secs\n",
      "\tforward prop: costs 0.03971457481384277 secs\n",
      "\tback prop: costs 0.4953594207763672 secs\n",
      "Iteration 12 (learning rate 2, iteration 60), cost: 3.1772202553145905, time: 0.5425136089324951\n",
      "\tconstruction: hidden layer dot costs 0.003000497817993164 secs\n",
      "\tconstruction: output layer dot costs 0.0010037422180175781 secs\n",
      "\tforward prop: costs 0.03959488868713379 secs\n",
      "\tback prop: costs 0.4954514503479004 secs\n",
      "Iteration 13 (learning rate 2, iteration 60), cost: 3.1558435950646273, time: 0.5420606136322021\n",
      "\tconstruction: hidden layer dot costs 0.004303455352783203 secs\n",
      "\tconstruction: output layer dot costs 0.0010001659393310547 secs\n",
      "\tforward prop: costs 0.04071545600891113 secs\n",
      "\tback prop: costs 0.4886469841003418 secs\n",
      "Iteration 14 (learning rate 2, iteration 60), cost: 3.1303427400875465, time: 0.5377287864685059\n",
      "\tconstruction: hidden layer dot costs 0.004314899444580078 secs\n",
      "\tconstruction: output layer dot costs 0.001001596450805664 secs\n",
      "\tforward prop: costs 0.03944516181945801 secs\n",
      "\tback prop: costs 0.4925658702850342 secs\n",
      "Iteration 15 (learning rate 2, iteration 60), cost: 3.10067041120143, time: 0.5402886867523193\n",
      "\tconstruction: hidden layer dot costs 0.005128145217895508 secs\n",
      "\tconstruction: output layer dot costs 0.0 secs\n",
      "\tforward prop: costs 0.048052310943603516 secs\n",
      "\tback prop: costs 0.48905158042907715 secs\n",
      "Iteration 16 (learning rate 2, iteration 60), cost: 3.0670168279141174, time: 0.5456783771514893\n",
      "\tconstruction: hidden layer dot costs 0.00399470329284668 secs\n",
      "\tconstruction: output layer dot costs 0.0 secs\n",
      "\tforward prop: costs 0.04070448875427246 secs\n",
      "\tback prop: costs 0.5116519927978516 secs\n",
      "Iteration 17 (learning rate 2, iteration 60), cost: 3.029620058974763, time: 0.5594470500946045\n",
      "\tconstruction: hidden layer dot costs 0.0054700374603271484 secs\n",
      "\tconstruction: output layer dot costs 0.0 secs\n",
      "\tforward prop: costs 0.044326066970825195 secs\n",
      "\tback prop: costs 0.5002157688140869 secs\n",
      "Iteration 18 (learning rate 2, iteration 60), cost: 2.9886698883958025, time: 0.5527133941650391\n",
      "\tconstruction: hidden layer dot costs 0.0039997100830078125 secs\n",
      "\tconstruction: output layer dot costs 0.0 secs\n",
      "\tforward prop: costs 0.040412187576293945 secs\n",
      "\tback prop: costs 0.4936952590942383 secs\n",
      "Iteration 19 (learning rate 2, iteration 60), cost: 2.94445460507008, time: 0.5417735576629639\n",
      "\tconstruction: hidden layer dot costs 0.0033960342407226562 secs\n",
      "\tconstruction: output layer dot costs 0.0010106563568115234 secs\n",
      "\tforward prop: costs 0.04050803184509277 secs\n",
      "\tback prop: costs 0.49469590187072754 secs\n",
      "Iteration 20 (learning rate 2, iteration 60), cost: 2.897588193638238, time: 0.5418727397918701\n",
      "\tconstruction: hidden layer dot costs 0.0034720897674560547 secs\n",
      "\tconstruction: output layer dot costs 0.0022292137145996094 secs\n",
      "\tforward prop: costs 0.0399479866027832 secs\n",
      "\tback prop: costs 0.4909048080444336 secs\n",
      "Iteration 21 (learning rate 2, iteration 60), cost: 2.8490164183991435, time: 0.5385611057281494\n",
      "\tconstruction: hidden layer dot costs 0.004007816314697266 secs\n",
      "\tconstruction: output layer dot costs 0.0010006427764892578 secs\n",
      "\tforward prop: costs 0.04015016555786133 secs\n",
      "\tback prop: costs 0.4994213581085205 secs\n",
      "Iteration 22 (learning rate 2, iteration 60), cost: 2.799728689561584, time: 0.5470097064971924\n",
      "\tconstruction: hidden layer dot costs 0.003996372222900391 secs\n",
      "\tconstruction: output layer dot costs 0.001001119613647461 secs\n",
      "\tforward prop: costs 0.04085111618041992 secs\n",
      "\tback prop: costs 0.49825263023376465 secs\n",
      "Iteration 23 (learning rate 2, iteration 60), cost: 2.7505505777518935, time: 0.5471043586730957\n",
      "\tconstruction: hidden layer dot costs 0.004088878631591797 secs\n",
      "\tconstruction: output layer dot costs 0.00099945068359375 secs\n",
      "\tforward prop: costs 0.04078173637390137 secs\n",
      "\tback prop: costs 0.4958381652832031 secs\n",
      "Iteration 24 (learning rate 2, iteration 60), cost: 2.7021322322648103, time: 0.5449995994567871\n",
      "\tconstruction: hidden layer dot costs 0.003325939178466797 secs\n",
      "\tconstruction: output layer dot costs 0.001003265380859375 secs\n",
      "\tforward prop: costs 0.04099726676940918 secs\n",
      "\tback prop: costs 0.4815328121185303 secs\n",
      "Iteration 25 (learning rate 2, iteration 60), cost: 2.654928284589135, time: 0.5303032398223877\n",
      "\tconstruction: hidden layer dot costs 0.004000186920166016 secs\n",
      "\tconstruction: output layer dot costs 0.0009999275207519531 secs\n",
      "\tforward prop: costs 0.05444169044494629 secs\n",
      "\tback prop: costs 0.49285292625427246 secs\n",
      "Iteration 26 (learning rate 2, iteration 60), cost: 2.609173105679668, time: 0.5542926788330078\n",
      "\tconstruction: hidden layer dot costs 0.0035085678100585938 secs\n",
      "\tconstruction: output layer dot costs 0.0011861324310302734 secs\n",
      "\tforward prop: costs 0.03933310508728027 secs\n",
      "\tback prop: costs 0.48008203506469727 secs\n",
      "Iteration 27 (learning rate 2, iteration 60), cost: 2.56490357313612, time: 0.5261166095733643\n",
      "\tconstruction: hidden layer dot costs 0.0029993057250976562 secs\n",
      "\tconstruction: output layer dot costs 0.002001047134399414 secs\n",
      "\tforward prop: costs 0.04344797134399414 secs\n",
      "\tback prop: costs 0.4941873550415039 secs\n",
      "Iteration 28 (learning rate 2, iteration 60), cost: 2.522016514892015, time: 0.5471677780151367\n",
      "\tconstruction: hidden layer dot costs 0.0043277740478515625 secs\n",
      "\tconstruction: output layer dot costs 0.0010006427764892578 secs\n",
      "\tforward prop: costs 0.040812015533447266 secs\n",
      "\tback prop: costs 0.47910380363464355 secs\n",
      "Iteration 29 (learning rate 2, iteration 60), cost: 2.480333944765635, time: 0.5272564888000488\n",
      "\tconstruction: hidden layer dot costs 0.004355430603027344 secs\n",
      "\tconstruction: output layer dot costs 0.001508951187133789 secs\n",
      "\tforward prop: costs 0.04024553298950195 secs\n",
      "\tback prop: costs 0.4921987056732178 secs\n",
      "Iteration 30 (learning rate 2, iteration 60), cost: 2.439660663853569, time: 0.5403108596801758\n",
      "\tconstruction: hidden layer dot costs 0.003999233245849609 secs\n",
      "\tconstruction: output layer dot costs 0.0011951923370361328 secs\n",
      "\tforward prop: costs 0.04673886299133301 secs\n",
      "\tback prop: costs 0.4871368408203125 secs\n",
      "Iteration 31 (learning rate 2, iteration 60), cost: 2.399832142489669, time: 0.5416421890258789\n",
      "\tconstruction: hidden layer dot costs 0.004057168960571289 secs\n",
      "\tconstruction: output layer dot costs 0.0013186931610107422 secs\n",
      "\tforward prop: costs 0.03925967216491699 secs\n",
      "\tback prop: costs 0.48160791397094727 secs\n",
      "Iteration 32 (learning rate 2, iteration 60), cost: 2.360748372299956, time: 0.529332160949707\n",
      "\tconstruction: hidden layer dot costs 0.004091978073120117 secs\n",
      "\tconstruction: output layer dot costs 0.001008749008178711 secs\n",
      "\tforward prop: costs 0.040955305099487305 secs\n",
      "\tback prop: costs 0.5076456069946289 secs\n",
      "Iteration 33 (learning rate 2, iteration 60), cost: 2.3223795360671113, time: 0.5568809509277344\n",
      "\tconstruction: hidden layer dot costs 0.003998756408691406 secs\n",
      "\tconstruction: output layer dot costs 0.0010008811950683594 secs\n",
      "\tforward prop: costs 0.03983616828918457 secs\n",
      "\tback prop: costs 0.4873061180114746 secs\n",
      "Iteration 34 (learning rate 2, iteration 60), cost: 2.2847464144574277, time: 0.5344345569610596\n",
      "\tconstruction: hidden layer dot costs 0.004201173782348633 secs\n",
      "\tconstruction: output layer dot costs 0.0 secs\n",
      "\tforward prop: costs 0.04051780700683594 secs\n",
      "\tback prop: costs 0.508544921875 secs\n",
      "Iteration 35 (learning rate 2, iteration 60), cost: 2.247918602908719, time: 0.5569705963134766\n",
      "\tconstruction: hidden layer dot costs 0.003000020980834961 secs\n",
      "\tconstruction: output layer dot costs 0.001001119613647461 secs\n",
      "\tforward prop: costs 0.04087972640991211 secs\n",
      "\tback prop: costs 0.4844365119934082 secs\n",
      "Iteration 36 (learning rate 2, iteration 60), cost: 2.212040291514363, time: 0.5324175357818604\n",
      "\tconstruction: hidden layer dot costs 0.0030868053436279297 secs\n",
      "\tconstruction: output layer dot costs 0.0010006427764892578 secs\n",
      "\tforward prop: costs 0.0408022403717041 secs\n",
      "\tback prop: costs 0.512495756149292 secs\n",
      "Iteration 37 (learning rate 2, iteration 60), cost: 2.177298089201957, time: 0.5594913959503174\n",
      "\tconstruction: hidden layer dot costs 0.004027366638183594 secs\n",
      "\tconstruction: output layer dot costs 0.0 secs\n",
      "\tforward prop: costs 0.0535578727722168 secs\n",
      "\tback prop: costs 0.519148588180542 secs\n",
      "Iteration 38 (learning rate 2, iteration 60), cost: 2.1438142281557373, time: 0.5810403823852539\n",
      "\tconstruction: hidden layer dot costs 0.004083156585693359 secs\n",
      "\tconstruction: output layer dot costs 0.0 secs\n",
      "\tforward prop: costs 0.0586550235748291 secs\n",
      "\tback prop: costs 0.483090877532959 secs\n",
      "Iteration 39 (learning rate 2, iteration 60), cost: 2.1115977668912307, time: 0.5490784645080566\n",
      "\tconstruction: hidden layer dot costs 0.0029985904693603516 secs\n",
      "\tconstruction: output layer dot costs 0.001046895980834961 secs\n",
      "\tforward prop: costs 0.041626691818237305 secs\n",
      "\tback prop: costs 0.506826639175415 secs\n",
      "Iteration 40 (learning rate 2, iteration 60), cost: 2.080589696165486, time: 0.555854082107544\n",
      "\tconstruction: hidden layer dot costs 0.004069089889526367 secs\n",
      "\tconstruction: output layer dot costs 0.0 secs\n",
      "\tforward prop: costs 0.0420992374420166 secs\n",
      "\tback prop: costs 0.5105867385864258 secs\n",
      "Iteration 41 (learning rate 2, iteration 60), cost: 2.0507140782273514, time: 0.5598819255828857\n",
      "\tconstruction: hidden layer dot costs 0.003997802734375 secs\n",
      "\tconstruction: output layer dot costs 0.0 secs\n",
      "\tforward prop: costs 0.04124760627746582 secs\n",
      "\tback prop: costs 0.4891245365142822 secs\n",
      "Iteration 42 (learning rate 2, iteration 60), cost: 2.021897479171625, time: 0.5374155044555664\n",
      "\tconstruction: hidden layer dot costs 0.004001140594482422 secs\n",
      "\tconstruction: output layer dot costs 0.0009980201721191406 secs\n",
      "\tforward prop: costs 0.04072093963623047 secs\n",
      "\tback prop: costs 0.49712586402893066 secs\n",
      "Iteration 43 (learning rate 2, iteration 60), cost: 1.9940708264172808, time: 0.5461006164550781\n",
      "\tconstruction: hidden layer dot costs 0.003999471664428711 secs\n",
      "\tconstruction: output layer dot costs 0.0010008811950683594 secs\n",
      "\tforward prop: costs 0.03939986228942871 secs\n",
      "\tback prop: costs 0.48362302780151367 secs\n",
      "Iteration 44 (learning rate 2, iteration 60), cost: 1.96716747489714, time: 0.5320112705230713\n",
      "\tconstruction: hidden layer dot costs 0.0043163299560546875 secs\n",
      "\tconstruction: output layer dot costs 0.0 secs\n",
      "\tforward prop: costs 0.04063582420349121 secs\n",
      "\tback prop: costs 0.508030891418457 secs\n",
      "Iteration 45 (learning rate 2, iteration 60), cost: 1.9411219820524845, time: 0.5560836791992188\n",
      "\tconstruction: hidden layer dot costs 0.004007577896118164 secs\n",
      "\tconstruction: output layer dot costs 0.0011181831359863281 secs\n",
      "\tforward prop: costs 0.039371490478515625 secs\n",
      "\tback prop: costs 0.4870798587799072 secs\n",
      "Iteration 46 (learning rate 2, iteration 60), cost: 1.915870046688008, time: 0.5335869789123535\n",
      "\tconstruction: hidden layer dot costs 0.004519939422607422 secs\n",
      "\tconstruction: output layer dot costs 0.0012981891632080078 secs\n",
      "\tforward prop: costs 0.0403292179107666 secs\n",
      "\tback prop: costs 0.4967648983001709 secs\n",
      "Iteration 47 (learning rate 2, iteration 60), cost: 1.8913491452035645, time: 0.5461776256561279\n",
      "\tconstruction: hidden layer dot costs 0.003998279571533203 secs\n",
      "\tconstruction: output layer dot costs 0.0 secs\n",
      "\tforward prop: costs 0.04053902626037598 secs\n",
      "\tback prop: costs 0.4845695495605469 secs\n",
      "Iteration 48 (learning rate 2, iteration 60), cost: 1.86749941223283, time: 0.531113862991333\n",
      "\tconstruction: hidden layer dot costs 0.004122495651245117 secs\n",
      "\tconstruction: output layer dot costs 0.0 secs\n",
      "\tforward prop: costs 0.040941476821899414 secs\n",
      "\tback prop: costs 0.48416996002197266 secs\n",
      "Iteration 49 (learning rate 2, iteration 60), cost: 1.844264485962081, time: 0.5327463150024414\n",
      "\tconstruction: hidden layer dot costs 0.0050466060638427734 secs\n",
      "\tconstruction: output layer dot costs 0.0 secs\n",
      "\tforward prop: costs 0.06412243843078613 secs\n",
      "\tback prop: costs 0.4844188690185547 secs\n",
      "Iteration 50 (learning rate 2, iteration 60), cost: 1.8215921755481823, time: 0.5558452606201172\n",
      "\tconstruction: hidden layer dot costs 0.004002094268798828 secs\n",
      "\tconstruction: output layer dot costs 0.0009987354278564453 secs\n",
      "\tforward prop: costs 0.04011344909667969 secs\n",
      "\tback prop: costs 0.4815025329589844 secs\n",
      "Iteration 51 (learning rate 2, iteration 60), cost: 1.7994348833008702, time: 0.5298609733581543\n",
      "\tconstruction: hidden layer dot costs 0.004000425338745117 secs\n",
      "\tconstruction: output layer dot costs 0.0 secs\n",
      "\tforward prop: costs 0.05108952522277832 secs\n",
      "\tback prop: costs 0.4951016902923584 secs\n",
      "Iteration 52 (learning rate 2, iteration 60), cost: 1.7777497451900341, time: 0.5532004833221436\n",
      "\tconstruction: hidden layer dot costs 0.003998756408691406 secs\n",
      "\tconstruction: output layer dot costs 0.0 secs\n",
      "\tforward prop: costs 0.04048418998718262 secs\n",
      "\tback prop: costs 0.4807705879211426 secs\n",
      "Iteration 53 (learning rate 2, iteration 60), cost: 1.7564984654149647, time: 0.5272641181945801\n",
      "\tconstruction: hidden layer dot costs 0.003998517990112305 secs\n",
      "\tconstruction: output layer dot costs 0.0010018348693847656 secs\n",
      "\tforward prop: costs 0.0393214225769043 secs\n",
      "\tback prop: costs 0.49532389640808105 secs\n",
      "Iteration 54 (learning rate 2, iteration 60), cost: 1.735646836685366, time: 0.5417056083679199\n",
      "\tconstruction: hidden layer dot costs 0.004000425338745117 secs\n",
      "\tconstruction: output layer dot costs 0.0010042190551757812 secs\n",
      "\tforward prop: costs 0.039995431900024414 secs\n",
      "\tback prop: costs 0.4846458435058594 secs\n",
      "Iteration 55 (learning rate 2, iteration 60), cost: 1.7151639719013532, time: 0.5316455364227295\n",
      "\tconstruction: hidden layer dot costs 0.003486156463623047 secs\n",
      "\tconstruction: output layer dot costs 0.0015053749084472656 secs\n",
      "\tforward prop: costs 0.03974175453186035 secs\n",
      "\tback prop: costs 0.49167919158935547 secs\n",
      "Iteration 56 (learning rate 2, iteration 60), cost: 1.6950213281676618, time: 0.538860559463501\n",
      "\tconstruction: hidden layer dot costs 0.00436854362487793 secs\n",
      "\tconstruction: output layer dot costs 0.0 secs\n",
      "\tforward prop: costs 0.039659976959228516 secs\n",
      "\tback prop: costs 0.49988341331481934 secs\n",
      "Iteration 57 (learning rate 2, iteration 60), cost: 1.6751916722596272, time: 0.5472450256347656\n",
      "\tconstruction: hidden layer dot costs 0.004002094268798828 secs\n",
      "\tconstruction: output layer dot costs 0.0009970664978027344 secs\n",
      "\tforward prop: costs 0.03999757766723633 secs\n",
      "\tback prop: costs 0.48996615409851074 secs\n",
      "Iteration 58 (learning rate 2, iteration 60), cost: 1.6556482029115205, time: 0.5369703769683838\n",
      "\tconstruction: hidden layer dot costs 0.0048182010650634766 secs\n",
      "\tconstruction: output layer dot costs 0.0 secs\n",
      "\tforward prop: costs 0.040853023529052734 secs\n",
      "\tback prop: costs 0.4857807159423828 secs\n",
      "Iteration 59 (learning rate 2, iteration 60), cost: 1.6363640957528316, time: 0.534552812576294\n",
      "\tconstruction: hidden layer dot costs 0.0030100345611572266 secs\n",
      "\tconstruction: output layer dot costs 0.0010025501251220703 secs\n",
      "\tforward prop: costs 0.040207862854003906 secs\n",
      "\tback prop: costs 0.5003843307495117 secs\n",
      "Iteration 60 (learning rate 2, iteration 60), cost: 1.6173127600980182, time: 0.5471291542053223\n",
      "Thời gian huấn luyện: 32.89 giây\n",
      "Độ chính xác trên tập test: 0.7410\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "# import theano\n",
    "# import theano.tensor as T\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Structure of the 3-layer neural network.\n",
    "Input_layer_size = 400\n",
    "Hidden_layer_size = 25\n",
    "Output_layer_size = 10\n",
    "\n",
    "# Matrix product function.  Default is to use CPU mode.\n",
    "Matrix_dot = np.dot\n",
    "\n",
    "\n",
    "def load_training_data(training_file='mnistdata.mat'):\n",
    "    '''Load training data (mnistdata.mat) and return (inputs, labels).\n",
    "\n",
    "    inputs: numpy array with size (5000, 400).\n",
    "    labels: numpy array with size (5000, 1).\n",
    "\n",
    "    The training data is from Andrew Ng's exercise of the Coursera\n",
    "    machine learning course (ex4data1.mat).\n",
    "    '''\n",
    "    training_data = sio.loadmat(training_file)\n",
    "    return (training_data['X'], training_data['y'])\n",
    "\n",
    "\n",
    "def load_weights(weight_file='mnistweights.mat'):\n",
    "    '''Load training data (mnistdata.mat) and return (inputs, labels).\n",
    "\n",
    "    The weights file is from Andrew Ng's exercise of the Coursera\n",
    "    machine learning course (ex4weights.mat).\n",
    "    '''\n",
    "    weights = sio.loadmat(weight_file)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def rand_init_weights(size_in, size_out):\n",
    "    epsilon_init = 0.12\n",
    "    return np.random.rand(size_out, 1 + size_in) * 2 * epsilon_init - epsilon_init\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + pow(math.e, -z))\n",
    "\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def gpu_matrix_dot():\n",
    "    time_start = time.time()\n",
    "    \n",
    "    x = np.random.rand(1000, 1000).astype(np.float32)\n",
    "    y = np.random.rand(1000, 1000).astype(np.float32)\n",
    "    z = np.dot(x, y)  # Nhân ma trận trong NumPy\n",
    "    \n",
    "    def f(x, y):\n",
    "        return np.dot(x, y)  # Hàm thực hiện nhân ma trận\n",
    "    \n",
    "    time_end = time.time()\n",
    "    print('NumPy function creation costs {} secs'.format(time_end - time_start))\n",
    "    \n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "def cost_function(theta1, theta2, input_layer_size, hidden_layer_size, output_layer_size, inputs, labels, regular=0):\n",
    "    '''\n",
    "    Note: theta1, theta2, inputs, labels are numpy arrays:\n",
    "\n",
    "        theta1: (25, 401)\n",
    "        theta2: (10, 26)\n",
    "        inputs: (5000, 400)\n",
    "        labels: (5000, 1)\n",
    "    '''\n",
    "    # construct neural network\n",
    "    input_layer = np.insert(inputs, 0, 1, axis=1)  # add bias, 5000x401\n",
    "\n",
    "    time_start = time.time()\n",
    "    hidden_layer = Matrix_dot(input_layer, theta1.T)\n",
    "    hidden_layer = sigmoid(hidden_layer)\n",
    "    hidden_layer = np.insert(hidden_layer, 0, 1, axis=1)  # add bias, 5000x26\n",
    "    time_end = time.time()\n",
    "    print('\\tconstruction: hidden layer dot costs {} secs'.format(time_end - time_start))\n",
    "\n",
    "    time_start = time.time()\n",
    "    output_layer = Matrix_dot(hidden_layer, theta2.T)  # 5000x10\n",
    "    output_layer = sigmoid(output_layer)\n",
    "    time_end = time.time()\n",
    "    print('\\tconstruction: output layer dot costs {} secs'.format(time_end - time_start))\n",
    "\n",
    "    # forward propagation: calculate cost\n",
    "    time_start = time.time()\n",
    "    cost = 0.0\n",
    "    for training_index in range(len(inputs)):\n",
    "        # transform label y[i] from a number to a vector.\n",
    "        #\n",
    "        # Note:\n",
    "        #   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        #    1  2  3  4  5  6  7  8  9 10\n",
    "        #\n",
    "        #   if y[i] is 0 -> [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "        #   if y[i] is 1 -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        outputs = [0] * output_layer_size\n",
    "        outputs[int(labels[training_index])-1] = 1\n",
    "\n",
    "        for k in range(output_layer_size):\n",
    "            cost += -outputs[k] * math.log(output_layer[training_index][k]) - (1 - outputs[k]) * math.log(1 - output_layer[training_index][k])\n",
    "    cost /= len(inputs)\n",
    "    time_end = time.time()\n",
    "    print('\\tforward prop: costs {} secs'.format(time_end - time_start))\n",
    "\n",
    "    # back propagation: calculate gradiants\n",
    "    time_start = time.time()\n",
    "    theta1_grad = np.zeros_like(theta1)  # 25x401\n",
    "    theta2_grad = np.zeros_like(theta2)  # 10x26\n",
    "    for index in range(len(inputs)):\n",
    "        # transform label y[i] from a number to a vector.\n",
    "        outputs = np.zeros((1, output_layer_size))  # (1,10)\n",
    "        outputs[0][labels[index]-1] = 1\n",
    "\n",
    "        # calculate delta3\n",
    "        delta3 = (output_layer[index] - outputs).T  # (10,1)\n",
    "\n",
    "        # calculate delta2\n",
    "        z2 = Matrix_dot(theta1, input_layer[index:index+1].T)  # (25,401) x (401,1)\n",
    "        z2 = np.insert(z2, 0, 1, axis=0)  # add bias, (26,1)\n",
    "        delta2 = np.multiply(\n",
    "            Matrix_dot(theta2.T, delta3),  # (26,10) x (10,1)\n",
    "            sigmoid_gradient(z2)  # (26,1)\n",
    "        )\n",
    "        delta2 = delta2[1:]  # (25,1)\n",
    "\n",
    "        # calculate gradients of theta1 and theta2\n",
    "        # (25,401) = (25,1) x (1,401)\n",
    "        theta1_grad += Matrix_dot(delta2, input_layer[index:index+1])\n",
    "        # (10,26) = (10,1) x (1,26)\n",
    "        theta2_grad += Matrix_dot(delta3, hidden_layer[index:index+1])\n",
    "    theta1_grad /= len(inputs)\n",
    "    theta2_grad /= len(inputs)\n",
    "    time_end = time.time()\n",
    "    print('\\tback prop: costs {} secs'.format(time_end - time_start))\n",
    "\n",
    "    return cost, (theta1_grad, theta2_grad)\n",
    "\n",
    "\n",
    "def gradient_descent(inputs, labels, learningrate=0.8, iteration=50):\n",
    "    '''\n",
    "    @return cost and trained model (weights).\n",
    "    '''\n",
    "    rand_theta1 = rand_init_weights(Input_layer_size, Hidden_layer_size)\n",
    "    rand_theta2 = rand_init_weights(Hidden_layer_size, Output_layer_size)\n",
    "    theta1 = rand_theta1\n",
    "    theta2 = rand_theta2\n",
    "    cost = 0.0\n",
    "    for i in range(iteration):\n",
    "        time_start = time.time()\n",
    "        cost, (theta1_grad, theta2_grad) = cost_function(theta1, theta2,\n",
    "            Input_layer_size, Hidden_layer_size, Output_layer_size,\n",
    "            inputs, labels, regular=0)\n",
    "        theta1 -= learningrate * theta1_grad\n",
    "        theta2 -= learningrate * theta2_grad\n",
    "        time_end = time.time()\n",
    "        print('Iteration {0} (learning rate {2}, iteration {3}), cost: {1}, time: {4}'.format(i+1, cost, learningrate, iteration, time_end - time_start))\n",
    "    return cost, (theta1, theta2)\n",
    "\n",
    "\n",
    "def train(inputs, labels, learningrate=0.8, iteration=50):\n",
    "    cost, model = gradient_descent(inputs, labels, learningrate, iteration)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, inputs):\n",
    "    theta1, theta2 = model\n",
    "    a1 = np.insert(inputs, 0, 1, axis=1)  # add bias, (5000,401)\n",
    "    a2 = np.dot(a1, theta1.T)  # (5000,401) x (401,25)\n",
    "    a2 = sigmoid(a2)\n",
    "    a2 = np.insert(a2, 0, 1, axis=1)  # add bias, (5000,26)\n",
    "    a3 = np.dot(a2, theta2.T)  # (5000,26) x (26,10)\n",
    "    a3 = sigmoid(a3)  # (5000,10)\n",
    "    return [i.argmax()+1 for i in a3]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gpu_mode = True\n",
    "    #gpu_mode = False\n",
    "    if gpu_mode is True:\n",
    "        print('GPU mode')\n",
    "        Matrix_dot = gpu_matrix_dot()\n",
    "    else:\n",
    "        print('CPU mode')\n",
    "        Matrix_dot = np.dot\n",
    "\n",
    "    # Note: There are 10 units which present the digits [1-9, 0]\n",
    "    # (in order) in the output layer.\n",
    "    inputs, labels = load_training_data()\n",
    "    # Chia dữ liệu thành 80% train và 20% test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    # (optional) load pre-trained model for debugging neural network construction\n",
    "    weights = load_weights()\n",
    "    theta1 = weights['Theta1']  # size: 25 entries, each has 401 numbers\n",
    "    theta2 = weights['Theta2']  # size: 10 entries, each has  26 numbers\n",
    "\n",
    "    # FIXME: Memory alignment issue in Scipy.\n",
    "    #   This issue leads Theano to complain that \"The numpy.ndarray\n",
    "    #   object is not aligned. Theano C code does not support that.\"\n",
    "    #\n",
    "    #   Related discussion: http://stackoverflow.com/questions/36321400/strange-typeerror-with-theano/36323861\n",
    "    # workaround to avoid memory alignment error in Scipy\n",
    "    theta1 = np.array(theta1)\n",
    "    theta2 = np.array(theta2)\n",
    "    #print('theta1: {}'.format(theta1))\n",
    "    #print('theta1 flags: {}'.format(theta1.flags))\n",
    "    #print('theta2: {}'.format(theta2))\n",
    "    #print('theta2 flags: {}'.format(theta2.flags))\n",
    " \n",
    "    cost, (theta1_grad, theta2_grad) = cost_function(theta1, theta2, Input_layer_size, Hidden_layer_size, Output_layer_size, inputs, labels, regular=0)\n",
    "    print('cost:', cost)\n",
    "\n",
    "    # train the model from scratch and predict based on it\n",
    "    # learning rate 0.10, iteration  60: 36% (cost: 3.217)\n",
    "    # learning rate 1.75, iteration  50: 77%\n",
    "    # learning rate 1.90, iteration  50: 75%\n",
    "    # learning rate 2.00, iteration  50: 82%\n",
    "    # learning rate 2.00, iteration 100: 87%\n",
    "    # learning rate 2.00, iteration 200: 93% (cost: 0.572)\n",
    "    # learning rate 2.00, iteration 300: 94% (cost: 0.485)\n",
    "    # learning rate 2.05, iteration  50: 79%\n",
    "    # learning rate 2.20, iteration  50: 64%\n",
    "    # Bắt đầu đo thời gian huấn luyện\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Huấn luyện mô hình trên tập train\n",
    "    model = train(X_train, y_train, learningrate=2, iteration=60)\n",
    "\n",
    "    # Kết thúc đo thời gian huấn luyện\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Tính tổng thời gian huấn luyện\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Thời gian huấn luyện: {training_time:.2f} giây\")\n",
    "    \n",
    "    outputs = predict(model, X_test)\n",
    "    # model = train(inputs, labels, learningrate=2, iteration=60)\n",
    "    #model = (theta1, theta2)\n",
    "    # outputs = predict(model, inputs)\n",
    "\n",
    "    correct_prediction = sum(outputs[i] == y_test[i][0] for i in range(len(y_test)))\n",
    "    precision = correct_prediction / len(y_test)\n",
    "\n",
    "    print(f'Độ chính xác trên tập test: {precision:.4f}')\n",
    "\n",
    "\n",
    "    # correct_prediction = 0\n",
    "    # for i, predict in enumerate(outputs):\n",
    "    #     if predict == labels[i][0]:\n",
    "    #         correct_prediction += 1\n",
    "    # precision = float(correct_prediction) / len(labels)\n",
    "    # print('precision: {}'.format(precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU mode\n",
      "NumPy function creation costs 0.03338813781738281 secs\n",
      "Parallelism: yes\n",
      "\tBcast theta1 and theta2 uses 0.0014274120330810547 secs.\n",
      "1\n",
      "\tScatter inputs uses 0.006052732467651367 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.0099029541015625 secs\n",
      "\tconstruction: output layer dot costs 0.0020132064819335938 secs\n",
      "\tforward prop: costs 0.07876014709472656 secs\n",
      "\tback prop: costs 0.8240408897399902 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 1 (learning rate 2, iteration 60), cost: 6.992027750572543, time: 0.9359138011932373\n",
      "1\n",
      "\tScatter inputs uses 0.004002809524536133 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.011059999465942383 secs\n",
      "\tconstruction: output layer dot costs 0.0019986629486083984 secs\n",
      "\tforward prop: costs 0.07767319679260254 secs\n",
      "\tback prop: costs 0.6969900131225586 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 2 (learning rate 2, iteration 60), cost: 5.282047062872304, time: 0.8072478771209717\n",
      "1\n",
      "\tScatter inputs uses 0.0024220943450927734 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.006493806838989258 secs\n",
      "\tconstruction: output layer dot costs 0.001001596450805664 secs\n",
      "\tforward prop: costs 0.06652617454528809 secs\n",
      "\tback prop: costs 0.6289865970611572 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 3 (learning rate 2, iteration 60), cost: 3.991765559867912, time: 0.7156238555908203\n",
      "1\n",
      "\tScatter inputs uses 0.0030014514923095703 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.0075283050537109375 secs\n",
      "\tconstruction: output layer dot costs 0.002264738082885742 secs\n",
      "\tforward prop: costs 0.06991052627563477 secs\n",
      "\tback prop: costs 0.5858688354492188 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 4 (learning rate 2, iteration 60), cost: 3.2719450991852113, time: 0.6765856742858887\n",
      "1\n",
      "\tScatter inputs uses 0.0019974708557128906 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.0071637630462646484 secs\n",
      "\tconstruction: output layer dot costs 0.0020093917846679688 secs\n",
      "\tforward prop: costs 0.054822683334350586 secs\n",
      "\tback prop: costs 0.6798152923583984 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 5 (learning rate 2, iteration 60), cost: 3.2500473204706988, time: 0.7548177242279053\n",
      "1\n",
      "\tScatter inputs uses 0.0022444725036621094 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.00660395622253418 secs\n",
      "\tconstruction: output layer dot costs 0.0025856494903564453 secs\n",
      "\tforward prop: costs 0.0539858341217041 secs\n",
      "\tback prop: costs 0.6671543121337891 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 6 (learning rate 2, iteration 60), cost: 3.23923161327945, time: 0.7398667335510254\n",
      "1\n",
      "\tScatter inputs uses 0.002000570297241211 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.00699615478515625 secs\n",
      "\tconstruction: output layer dot costs 0.0019960403442382812 secs\n",
      "\tforward prop: costs 0.07559084892272949 secs\n",
      "\tback prop: costs 0.6359694004058838 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 7 (learning rate 2, iteration 60), cost: 3.22904754516722, time: 0.7323360443115234\n",
      "1\n",
      "\tScatter inputs uses 0.0029993057250976562 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.008511066436767578 secs\n",
      "\tconstruction: output layer dot costs 0.0025746822357177734 secs\n",
      "\tforward prop: costs 0.06920266151428223 secs\n",
      "\tback prop: costs 0.6321594715118408 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 8 (learning rate 2, iteration 60), cost: 3.218186831262093, time: 0.7214658260345459\n",
      "1\n",
      "\tScatter inputs uses 0.0028619766235351562 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.0070743560791015625 secs\n",
      "\tconstruction: output layer dot costs 0.002419710159301758 secs\n",
      "\tforward prop: costs 0.06593966484069824 secs\n",
      "\tback prop: costs 0.6011676788330078 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 9 (learning rate 2, iteration 60), cost: 3.206075695835945, time: 0.6867263317108154\n",
      "1\n",
      "\tScatter inputs uses 0.0024673938751220703 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.007256269454956055 secs\n",
      "\tconstruction: output layer dot costs 0.002460002899169922 secs\n",
      "\tforward prop: costs 0.056914567947387695 secs\n",
      "\tback prop: costs 0.597954273223877 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 10 (learning rate 2, iteration 60), cost: 3.192254384922386, time: 0.6740972995758057\n",
      "1\n",
      "\tScatter inputs uses 0.0019969940185546875 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.007008790969848633 secs\n",
      "\tconstruction: output layer dot costs 0.0019986629486083984 secs\n",
      "\tforward prop: costs 0.05325508117675781 secs\n",
      "\tback prop: costs 0.5997672080993652 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 11 (learning rate 2, iteration 60), cost: 3.1762979314382878, time: 0.6710376739501953\n",
      "1\n",
      "\tScatter inputs uses 0.0023415088653564453 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.0067975521087646484 secs\n",
      "\tconstruction: output layer dot costs 0.0020012855529785156 secs\n",
      "\tforward prop: costs 0.06397771835327148 secs\n",
      "\tback prop: costs 0.5873022079467773 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 12 (learning rate 2, iteration 60), cost: 3.157835891638561, time: 0.6704554557800293\n",
      "1\n",
      "\tScatter inputs uses 0.0019998550415039062 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.007181644439697266 secs\n",
      "\tconstruction: output layer dot costs 0.001001119613647461 secs\n",
      "\tforward prop: costs 0.07673048973083496 secs\n",
      "\tback prop: costs 0.6107931137084961 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 13 (learning rate 2, iteration 60), cost: 3.1365908002102714, time: 0.7070024013519287\n",
      "1\n",
      "\tScatter inputs uses 0.00251007080078125 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.009070634841918945 secs\n",
      "\tconstruction: output layer dot costs 0.0009970664978027344 secs\n",
      "\tforward prop: costs 0.0616765022277832 secs\n",
      "\tback prop: costs 0.5906567573547363 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 14 (learning rate 2, iteration 60), cost: 3.112397733776697, time: 0.671933650970459\n",
      "1\n",
      "\tScatter inputs uses 0.0029993057250976562 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.007948160171508789 secs\n",
      "\tconstruction: output layer dot costs 0.001001119613647461 secs\n",
      "\tforward prop: costs 0.055161476135253906 secs\n",
      "\tback prop: costs 0.5789954662322998 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 15 (learning rate 2, iteration 60), cost: 3.0851901092366996, time: 0.6531145572662354\n",
      "1\n",
      "\tScatter inputs uses 0.0030024051666259766 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.007253170013427734 secs\n",
      "\tconstruction: output layer dot costs 0.001996278762817383 secs\n",
      "\tforward prop: costs 0.05389237403869629 secs\n",
      "\tback prop: costs 0.5855474472045898 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 16 (learning rate 2, iteration 60), cost: 3.054969039395688, time: 0.6591091156005859\n",
      "1\n",
      "\tScatter inputs uses 0.003189563751220703 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.007653713226318359 secs\n",
      "\tconstruction: output layer dot costs 0.0013217926025390625 secs\n",
      "\tforward prop: costs 0.06278729438781738 secs\n",
      "\tback prop: costs 0.5815742015838623 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 17 (learning rate 2, iteration 60), cost: 3.0217878494563073, time: 0.6625423431396484\n",
      "1\n",
      "\tScatter inputs uses 0.001997709274291992 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.007002115249633789 secs\n",
      "\tconstruction: output layer dot costs 0.0009982585906982422 secs\n",
      "\tforward prop: costs 0.05430269241333008 secs\n",
      "\tback prop: costs 0.6120004653930664 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 18 (learning rate 2, iteration 60), cost: 2.9857718926729513, time: 0.6834542751312256\n",
      "1\n",
      "\tScatter inputs uses 0.0029997825622558594 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.006882905960083008 secs\n",
      "\tconstruction: output layer dot costs 0.0010001659393310547 secs\n",
      "\tforward prop: costs 0.05487656593322754 secs\n",
      "\tback prop: costs 0.5844099521636963 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 19 (learning rate 2, iteration 60), cost: 2.9471736056350224, time: 0.6581811904907227\n",
      "1\n",
      "\tScatter inputs uses 0.002012968063354492 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.007231473922729492 secs\n",
      "\tconstruction: output layer dot costs 0.0017254352569580078 secs\n",
      "\tforward prop: costs 0.06414580345153809 secs\n",
      "\tback prop: costs 0.59279465675354 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 20 (learning rate 2, iteration 60), cost: 2.9064242559792275, time: 0.6759636402130127\n",
      "1\n",
      "\tScatter inputs uses 0.003010988235473633 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.005997180938720703 secs\n",
      "\tconstruction: output layer dot costs 0.0017671585083007812 secs\n",
      "\tforward prop: costs 0.05610227584838867 secs\n",
      "\tback prop: costs 0.5682373046875 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 21 (learning rate 2, iteration 60), cost: 2.8640912431990078, time: 0.6431069374084473\n",
      "1\n",
      "\tScatter inputs uses 0.0031478404998779297 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.007487058639526367 secs\n",
      "\tconstruction: output layer dot costs 0.0009999275207519531 secs\n",
      "\tforward prop: costs 0.06204390525817871 secs\n",
      "\tback prop: costs 0.5742931365966797 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 22 (learning rate 2, iteration 60), cost: 2.820705691657999, time: 0.6553406715393066\n",
      "1\n",
      "\tScatter inputs uses 0.0033097267150878906 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.00600743293762207 secs\n",
      "\tconstruction: output layer dot costs 0.0010006427764892578 secs\n",
      "\tforward prop: costs 0.05434679985046387 secs\n",
      "\tback prop: costs 0.5771002769470215 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 23 (learning rate 2, iteration 60), cost: 2.776621969400834, time: 0.6480863094329834\n",
      "1\n",
      "\tScatter inputs uses 0.0019979476928710938 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.007029056549072266 secs\n",
      "\tconstruction: output layer dot costs 0.0013284683227539062 secs\n",
      "\tforward prop: costs 0.055698394775390625 secs\n",
      "\tback prop: costs 0.5898807048797607 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 24 (learning rate 2, iteration 60), cost: 2.732039009109544, time: 0.6629927158355713\n",
      "1\n",
      "\tScatter inputs uses 0.0028018951416015625 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.007535457611083984 secs\n",
      "\tconstruction: output layer dot costs 0.001993417739868164 secs\n",
      "\tforward prop: costs 0.052867889404296875 secs\n",
      "\tback prop: costs 0.5993208885192871 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 25 (learning rate 2, iteration 60), cost: 2.687053541099971, time: 0.67057204246521\n",
      "1\n",
      "\tScatter inputs uses 0.0024976730346679688 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.006045818328857422 secs\n",
      "\tconstruction: output layer dot costs 0.001997232437133789 secs\n",
      "\tforward prop: costs 0.054718017578125 secs\n",
      "\tback prop: costs 0.581085205078125 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 26 (learning rate 2, iteration 60), cost: 2.6416442570331653, time: 0.6548771858215332\n",
      "1\n",
      "\tScatter inputs uses 0.0019998550415039062 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.006832122802734375 secs\n",
      "\tconstruction: output layer dot costs 0.0019991397857666016 secs\n",
      "\tforward prop: costs 0.0691378116607666 secs\n",
      "\tback prop: costs 0.5909664630889893 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 27 (learning rate 2, iteration 60), cost: 2.595688209341321, time: 0.6786098480224609\n",
      "1\n",
      "\tScatter inputs uses 0.0019996166229248047 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.00789499282836914 secs\n",
      "\tconstruction: output layer dot costs 0.001146554946899414 secs\n",
      "\tforward prop: costs 0.05431699752807617 secs\n",
      "\tback prop: costs 0.5859322547912598 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 28 (learning rate 2, iteration 60), cost: 2.5490945378084287, time: 0.6583597660064697\n",
      "1\n",
      "\tScatter inputs uses 0.001998424530029297 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.006999969482421875 secs\n",
      "\tconstruction: output layer dot costs 0.0014138221740722656 secs\n",
      "\tforward prop: costs 0.0526735782623291 secs\n",
      "\tback prop: costs 0.5810861587524414 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 29 (learning rate 2, iteration 60), cost: 2.50200475452939, time: 0.6509711742401123\n",
      "1\n",
      "\tScatter inputs uses 0.0021333694458007812 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.007463693618774414 secs\n",
      "\tconstruction: output layer dot costs 0.0019981861114501953 secs\n",
      "\tforward prop: costs 0.06365585327148438 secs\n",
      "\tback prop: costs 0.5942621231079102 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 30 (learning rate 2, iteration 60), cost: 2.454862660817229, time: 0.6777856349945068\n",
      "1\n",
      "\tScatter inputs uses 0.001999378204345703 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.006005048751831055 secs\n",
      "\tconstruction: output layer dot costs 0.0013430118560791016 secs\n",
      "\tforward prop: costs 0.05418109893798828 secs\n",
      "\tback prop: costs 0.6086573600769043 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 31 (learning rate 2, iteration 60), cost: 2.408184194022738, time: 0.679236888885498\n",
      "1\n",
      "\tScatter inputs uses 0.001993894577026367 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.007546424865722656 secs\n",
      "\tconstruction: output layer dot costs 0.002351045608520508 secs\n",
      "\tforward prop: costs 0.06453919410705566 secs\n",
      "\tback prop: costs 0.6208181381225586 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 32 (learning rate 2, iteration 60), cost: 2.3623066779552064, time: 0.7072873115539551\n",
      "1\n",
      "\tScatter inputs uses 0.001998424530029297 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.007002115249633789 secs\n",
      "\tconstruction: output layer dot costs 0.0014712810516357422 secs\n",
      "\tforward prop: costs 0.053198814392089844 secs\n",
      "\tback prop: costs 0.6155562400817871 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 33 (learning rate 2, iteration 60), cost: 2.317404684898555, time: 0.6877386569976807\n",
      "1\n",
      "\tScatter inputs uses 0.0019991397857666016 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.007027626037597656 secs\n",
      "\tconstruction: output layer dot costs 0.0021562576293945312 secs\n",
      "\tforward prop: costs 0.05389761924743652 secs\n",
      "\tback prop: costs 0.6276445388793945 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 34 (learning rate 2, iteration 60), cost: 2.273599691678697, time: 0.6997280120849609\n",
      "1\n",
      "\tScatter inputs uses 0.002002716064453125 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.00679779052734375 secs\n",
      "\tconstruction: output layer dot costs 0.001392364501953125 secs\n",
      "\tforward prop: costs 0.054460763931274414 secs\n",
      "\tback prop: costs 0.6152572631835938 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 35 (learning rate 2, iteration 60), cost: 2.2310112677382565, time: 0.6873090267181396\n",
      "1\n",
      "\tScatter inputs uses 0.0034580230712890625 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.00648951530456543 secs\n",
      "\tconstruction: output layer dot costs 0.0020618438720703125 secs\n",
      "\tforward prop: costs 0.0622560977935791 secs\n",
      "\tback prop: costs 0.6114299297332764 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 36 (learning rate 2, iteration 60), cost: 2.189766286912428, time: 0.6968777179718018\n",
      "1\n",
      "\tScatter inputs uses 0.0030002593994140625 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.007758378982543945 secs\n",
      "\tconstruction: output layer dot costs 0.0016937255859375 secs\n",
      "\tforward prop: costs 0.06931543350219727 secs\n",
      "\tback prop: costs 0.5846447944641113 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 37 (learning rate 2, iteration 60), cost: 2.149993213176821, time: 0.6734166145324707\n",
      "1\n",
      "\tScatter inputs uses 0.0020020008087158203 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.006272554397583008 secs\n",
      "\tconstruction: output layer dot costs 0.002244234085083008 secs\n",
      "\tforward prop: costs 0.054186344146728516 secs\n",
      "\tback prop: costs 0.6291940212249756 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 38 (learning rate 2, iteration 60), cost: 2.1118015164421067, time: 0.7019205093383789\n",
      "1\n",
      "\tScatter inputs uses 0.002997875213623047 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.008002281188964844 secs\n",
      "\tconstruction: output layer dot costs 0.0019998550415039062 secs\n",
      "\tforward prop: costs 0.054793596267700195 secs\n",
      "\tback prop: costs 0.5905375480651855 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 39 (learning rate 2, iteration 60), cost: 2.0752476219889373, time: 0.6658198833465576\n",
      "1\n",
      "\tScatter inputs uses 0.0019974708557128906 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.0073854923248291016 secs\n",
      "\tconstruction: output layer dot costs 0.0019965171813964844 secs\n",
      "\tforward prop: costs 0.054506778717041016 secs\n",
      "\tback prop: costs 0.5843017101287842 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 40 (learning rate 2, iteration 60), cost: 2.040310840485803, time: 0.6583976745605469\n",
      "1\n",
      "\tScatter inputs uses 0.0031890869140625 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.008371829986572266 secs\n",
      "\tconstruction: output layer dot costs 0.0009982585906982422 secs\n",
      "\tforward prop: costs 0.05648231506347656 secs\n",
      "\tback prop: costs 0.6258106231689453 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 41 (learning rate 2, iteration 60), cost: 2.0069005542803664, time: 0.7040207386016846\n",
      "1\n",
      "\tScatter inputs uses 0.0022382736206054688 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.0071392059326171875 secs\n",
      "\tconstruction: output layer dot costs 0.0010051727294921875 secs\n",
      "\tforward prop: costs 0.06412911415100098 secs\n",
      "\tback prop: costs 0.5819244384765625 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 42 (learning rate 2, iteration 60), cost: 1.9748852973976005, time: 0.6650824546813965\n",
      "1\n",
      "\tScatter inputs uses 0.001998424530029297 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.007002830505371094 secs\n",
      "\tconstruction: output layer dot costs 0.0019989013671875 secs\n",
      "\tforward prop: costs 0.05428647994995117 secs\n",
      "\tback prop: costs 0.6056268215179443 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 43 (learning rate 2, iteration 60), cost: 1.9441210901216313, time: 0.6789209842681885\n",
      "1\n",
      "\tScatter inputs uses 0.0030019283294677734 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.00599980354309082 secs\n",
      "\tconstruction: output layer dot costs 0.0020012855529785156 secs\n",
      "\tforward prop: costs 0.053414344787597656 secs\n",
      "\tback prop: costs 0.5773148536682129 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 44 (learning rate 2, iteration 60), cost: 1.9144682415360268, time: 0.648740291595459\n",
      "1\n",
      "\tScatter inputs uses 0.002231597900390625 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.00640869140625 secs\n",
      "\tconstruction: output layer dot costs 0.0014650821685791016 secs\n",
      "\tforward prop: costs 0.06006217002868652 secs\n",
      "\tback prop: costs 0.5940310955047607 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 45 (learning rate 2, iteration 60), cost: 1.8857979105631435, time: 0.6738729476928711\n",
      "1\n",
      "\tScatter inputs uses 0.0019953250885009766 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.0059967041015625 secs\n",
      "\tconstruction: output layer dot costs 0.0020635128021240234 secs\n",
      "\tforward prop: costs 0.05265069007873535 secs\n",
      "\tback prop: costs 0.6349020004272461 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 46 (learning rate 2, iteration 60), cost: 1.8579931023944092, time: 0.7060964107513428\n",
      "1\n",
      "\tScatter inputs uses 0.0029990673065185547 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.006998538970947266 secs\n",
      "\tconstruction: output layer dot costs 0.002212047576904297 secs\n",
      "\tforward prop: costs 0.05578732490539551 secs\n",
      "\tback prop: costs 0.5799386501312256 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 47 (learning rate 2, iteration 60), cost: 1.8309477689611757, time: 0.6539874076843262\n",
      "1\n",
      "\tScatter inputs uses 0.0019986629486083984 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.00700068473815918 secs\n",
      "\tconstruction: output layer dot costs 0.0020008087158203125 secs\n",
      "\tforward prop: costs 0.06002998352050781 secs\n",
      "\tback prop: costs 0.6078472137451172 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 48 (learning rate 2, iteration 60), cost: 1.8045659412275739, time: 0.6870496273040771\n",
      "1\n",
      "\tScatter inputs uses 0.001998424530029297 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.006999015808105469 secs\n",
      "\tconstruction: output layer dot costs 0.0012669563293457031 secs\n",
      "\tforward prop: costs 0.05569291114807129 secs\n",
      "\tback prop: costs 0.6065311431884766 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 49 (learning rate 2, iteration 60), cost: 1.778761549599441, time: 0.6815016269683838\n",
      "1\n",
      "\tScatter inputs uses 0.001999378204345703 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.0070002079010009766 secs\n",
      "\tconstruction: output layer dot costs 0.0010046958923339844 secs\n",
      "\tforward prop: costs 0.053864240646362305 secs\n",
      "\tback prop: costs 0.6447443962097168 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 50 (learning rate 2, iteration 60), cost: 1.7534589798607936, time: 0.7156755924224854\n",
      "1\n",
      "\tScatter inputs uses 0.0020096302032470703 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.006125450134277344 secs\n",
      "\tconstruction: output layer dot costs 0.002001047134399414 secs\n",
      "\tforward prop: costs 0.053780317306518555 secs\n",
      "\tback prop: costs 0.5967669486999512 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 51 (learning rate 2, iteration 60), cost: 1.7285942643961607, time: 0.670151948928833\n",
      "1\n",
      "\tScatter inputs uses 0.0019998550415039062 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.00599980354309082 secs\n",
      "\tconstruction: output layer dot costs 0.001001596450805664 secs\n",
      "\tforward prop: costs 0.054268598556518555 secs\n",
      "\tback prop: costs 0.5792539119720459 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 52 (learning rate 2, iteration 60), cost: 1.704116778761271, time: 0.6496286392211914\n",
      "1\n",
      "\tScatter inputs uses 0.0030007362365722656 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.005997896194458008 secs\n",
      "\tconstruction: output layer dot costs 0.0024917125701904297 secs\n",
      "\tforward prop: costs 0.05422210693359375 secs\n",
      "\tback prop: costs 0.5667765140533447 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 53 (learning rate 2, iteration 60), cost: 1.6799911337604976, time: 0.6390008926391602\n",
      "1\n",
      "\tScatter inputs uses 0.0019998550415039062 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.005998849868774414 secs\n",
      "\tconstruction: output layer dot costs 0.002002239227294922 secs\n",
      "\tforward prop: costs 0.052999258041381836 secs\n",
      "\tback prop: costs 0.7320823669433594 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 54 (learning rate 2, iteration 60), cost: 1.656198567112407, time: 0.8026058673858643\n",
      "1\n",
      "\tScatter inputs uses 0.0030088424682617188 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.0070116519927978516 secs\n",
      "\tconstruction: output layer dot costs 0.001004934310913086 secs\n",
      "\tforward prop: costs 0.05539250373840332 secs\n",
      "\tback prop: costs 0.7426910400390625 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 55 (learning rate 2, iteration 60), cost: 1.6327367722606498, time: 0.819380521774292\n",
      "1\n",
      "\tScatter inputs uses 0.004072427749633789 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.007229328155517578 secs\n",
      "\tconstruction: output layer dot costs 0.0010018348693847656 secs\n",
      "\tforward prop: costs 0.06128287315368652 secs\n",
      "\tback prop: costs 0.6626832485198975 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 56 (learning rate 2, iteration 60), cost: 1.6096172110745293, time: 0.7434430122375488\n",
      "1\n",
      "\tScatter inputs uses 0.002997875213623047 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.008043050765991211 secs\n",
      "\tconstruction: output layer dot costs 0.0010106563568115234 secs\n",
      "\tforward prop: costs 0.05473470687866211 secs\n",
      "\tback prop: costs 0.58272385597229 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 57 (learning rate 2, iteration 60), cost: 1.586859873857951, time: 0.6576006412506104\n",
      "1\n",
      "\tScatter inputs uses 0.0020096302032470703 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.006382465362548828 secs\n",
      "\tconstruction: output layer dot costs 0.0010039806365966797 secs\n",
      "\tforward prop: costs 0.0530085563659668 secs\n",
      "\tback prop: costs 0.5864341259002686 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 58 (learning rate 2, iteration 60), cost: 1.5644868304921664, time: 0.6552917957305908\n",
      "1\n",
      "\tScatter inputs uses 0.0019998550415039062 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.00601506233215332 secs\n",
      "\tconstruction: output layer dot costs 0.002172708511352539 secs\n",
      "\tforward prop: costs 0.053437232971191406 secs\n",
      "\tback prop: costs 0.6011977195739746 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 59 (learning rate 2, iteration 60), cost: 1.5425167248506677, time: 0.6730561256408691\n",
      "1\n",
      "\tScatter inputs uses 0.0029993057250976562 secs.\n",
      "\tScatter labels uses 0.0 secs.\n",
      "\tconstruction: hidden layer dot costs 0.007313728332519531 secs\n",
      "\tconstruction: output layer dot costs 0.0010101795196533203 secs\n",
      "\tforward prop: costs 0.05351853370666504 secs\n",
      "\tback prop: costs 0.5950615406036377 secs\n",
      "\tGather theta1 uses 0.0 secs.\n",
      "\tGather theta2 uses 0.0 secs.\n",
      "Iteration 60 (learning rate 2, iteration 60), cost: 1.5209618429737732, time: 0.6662721633911133\n",
      "precision: 0.8192\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import functools\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import time\n",
    "\n",
    "from mpi4py import MPI\n",
    "import os\n",
    "\n",
    "# Thiết lập biến môi trường để đảm bảo rẽ nhánh True\n",
    "os.environ['MNISTNN_GPU'] = 'yes'\n",
    "os.environ['MNISTNN_PARALLEL'] = 'yes'\n",
    "\n",
    "if os.getenv('MNISTNN_GPU') == 'yes':\n",
    "    Gpu_mode = True\n",
    "else:\n",
    "    Gpu_mode = False\n",
    "\n",
    "if os.getenv('MNISTNN_PARALLEL') == 'yes':\n",
    "    Distributed = True\n",
    "else:\n",
    "    Distributed = False\n",
    "\n",
    "# if Gpu_mode is True:\n",
    "#     import theano\n",
    "#     import theano.tensor as T\n",
    "\n",
    "\n",
    "# Init MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "# Structure of the 3-layer neural network.\n",
    "Input_layer_size = 400\n",
    "Hidden_layer_size = 25\n",
    "Output_layer_size = 10\n",
    "\n",
    "# Matrix product function.  Default is to use CPU mode.\n",
    "Matrix_dot = np.dot\n",
    "\n",
    "\n",
    "def convert_memory_ordering_f2c(array):\n",
    "    if np.isfortran(array) is True:\n",
    "        return np.ascontiguousarray(array)\n",
    "    else:\n",
    "        return array\n",
    "\n",
    "\n",
    "def load_training_data(training_file='mnistdata.mat'):\n",
    "    '''Load training data (mnistdata.mat) and return (inputs, labels).\n",
    "\n",
    "    inputs: numpy array with size (5000, 400).\n",
    "    labels: numpy array with size (5000, 1).\n",
    "\n",
    "    The training data is from Andrew Ng's exercise of the Coursera\n",
    "    machine learning course (ex4data1.mat).\n",
    "    '''\n",
    "  \n",
    "    training_data = sio.loadmat(training_file)\n",
    "    inputs = training_data['X'].astype('f8')\n",
    "    inputs = convert_memory_ordering_f2c(inputs)\n",
    "    labels = training_data['y'].reshape(training_data['y'].shape[0])\n",
    "    labels = convert_memory_ordering_f2c(labels)\n",
    "    return (inputs, labels)\n",
    "\n",
    "\n",
    "def load_weights(weight_file='mnistweights.mat'):\n",
    "    '''Load training data (mnistdata.mat) and return (inputs, labels).\n",
    "\n",
    "    The weights file is from Andrew Ng's exercise of the Coursera\n",
    "    machine learning course (ex4weights.mat).\n",
    "    '''\n",
    "    weights = sio.loadmat(weight_file)\n",
    "    theta1 = convert_memory_ordering_f2c(weights['Theta1'].astype('f8'))  # size: 25 entries, each has 401 numbers\n",
    "    theta2 = convert_memory_ordering_f2c(weights['Theta2'].astype('f8'))  # size: 10 entries, each has  26 numbers\n",
    "    return (theta1, theta2)\n",
    "\n",
    "\n",
    "def rand_init_weights(size_in, size_out):\n",
    "    epsilon_init = 0.12\n",
    "    return np.random.rand(size_out, 1 + size_in) * 2 * epsilon_init - epsilon_init\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + pow(math.e, -z))\n",
    "\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "\n",
    "if Gpu_mode is True:\n",
    "    def gpu_matrix_dot():\n",
    "        time_start = time.time()\n",
    "    \n",
    "        x = np.random.rand(1000, 1000).astype(np.float32)\n",
    "        y = np.random.rand(1000, 1000).astype(np.float32)\n",
    "        z = np.dot(x, y)  # Nhân ma trận trong NumPy\n",
    "        \n",
    "        def f(x, y):\n",
    "            return np.dot(x, y)  # Hàm thực hiện nhân ma trận\n",
    "        \n",
    "        time_end = time.time()\n",
    "        print('NumPy function creation costs {} secs'.format(time_end - time_start))\n",
    "        return f\n",
    "else:\n",
    "    def gpu_matrix_dot():\n",
    "        pass\n",
    "\n",
    "\n",
    "def cost_function(theta1, theta2, input_layer_size, hidden_layer_size, output_layer_size, inputs, labels, regular=0):\n",
    "    '''\n",
    "    Note: theta1, theta2, inputs, labels are numpy arrays:\n",
    "\n",
    "        theta1: (25, 401)\n",
    "        theta2: (10, 26)\n",
    "        inputs: (5000, 400)\n",
    "        labels: (5000, 1)\n",
    "    '''\n",
    "    input_layer = np.insert(inputs, 0, 1, axis=1)  # add bias, 5000x401\n",
    "\n",
    "    time_start = time.time()\n",
    "    hidden_layer = Matrix_dot(input_layer, theta1.T)\n",
    "    hidden_layer = sigmoid(hidden_layer)\n",
    "    hidden_layer = np.insert(hidden_layer, 0, 1, axis=1)  # add bias, 5000x26\n",
    "    time_end = time.time()\n",
    "    if comm.rank == 0:\n",
    "        print('\\tconstruction: hidden layer dot costs {} secs'.format(time_end - time_start))\n",
    "\n",
    "    time_start = time.time()\n",
    "    output_layer = Matrix_dot(hidden_layer, theta2.T)  # 5000x10\n",
    "    output_layer = sigmoid(output_layer)\n",
    "    time_end = time.time()\n",
    "    if comm.rank == 0:\n",
    "        print('\\tconstruction: output layer dot costs {} secs'.format(time_end - time_start))\n",
    "\n",
    "    # forward propagation: calculate cost\n",
    "    time_start = time.time()\n",
    "    cost = 0.0\n",
    "    for training_index in range(len(inputs)):\n",
    "        # transform label y[i] from a number to a vector.\n",
    "        #\n",
    "        # Note:\n",
    "        #   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        #    1  2  3  4  5  6  7  8  9 10\n",
    "        #\n",
    "        #   if y[i] is 0 -> [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "        #   if y[i] is 1 -> [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        outputs = [0] * output_layer_size\n",
    "        outputs[labels[training_index]-1] = 1\n",
    "\n",
    "        for k in range(output_layer_size):\n",
    "            error = -outputs[k] * math.log(output_layer[training_index][k]) - (1 - outputs[k]) * math.log(1 - output_layer[training_index][k])\n",
    "            cost += error\n",
    "    cost /= len(inputs)\n",
    "    time_end = time.time()\n",
    "    if comm.rank == 0:\n",
    "        print('\\tforward prop: costs {} secs'.format(time_end - time_start))\n",
    "\n",
    "    # back propagation: calculate gradiants\n",
    "    time_start = time.time()\n",
    "    theta1_grad = np.zeros_like(theta1)  # 25x401\n",
    "    theta2_grad = np.zeros_like(theta2)  # 10x26\n",
    "    for index in range(len(inputs)):\n",
    "        # transform label y[i] from a number to a vector.\n",
    "        outputs = np.zeros((1, output_layer_size))  # (1,10)\n",
    "        outputs[0][labels[index]-1] = 1\n",
    "\n",
    "        # calculate delta3\n",
    "        delta3 = (output_layer[index] - outputs).T  # (10,1)\n",
    "\n",
    "        # calculate delta2\n",
    "        z2 = Matrix_dot(theta1, input_layer[index:index+1].T)  # (25,401) x (401,1)\n",
    "        z2 = np.insert(z2, 0, 1, axis=0)  # add bias, (26,1)\n",
    "        delta2 = np.multiply(\n",
    "            Matrix_dot(theta2.T, delta3),  # (26,10) x (10,1)\n",
    "            sigmoid_gradient(z2)  # (26,1)\n",
    "        )\n",
    "        delta2 = delta2[1:]  # (25,1)\n",
    "\n",
    "        # calculate gradients of theta1 and theta2\n",
    "        # (25,401) = (25,1) x (1,401)\n",
    "        theta1_grad += Matrix_dot(delta2, input_layer[index:index+1])\n",
    "        # (10,26) = (10,1) x (1,26)\n",
    "        theta2_grad += Matrix_dot(delta3, hidden_layer[index:index+1])\n",
    "    theta1_grad /= len(inputs)\n",
    "    theta2_grad /= len(inputs)\n",
    "    time_end = time.time()\n",
    "    if comm.rank == 0:\n",
    "        print('\\tback prop: costs {} secs'.format(time_end - time_start))\n",
    "\n",
    "    return cost, (theta1_grad, theta2_grad)\n",
    "\n",
    "\n",
    "def gradient_descent(inputs, labels, learningrate=0.8, iteration=50):\n",
    "    '''\n",
    "    @return cost and trained model (weights).\n",
    "    '''\n",
    "    if Distributed is True:\n",
    "        if comm.rank == 0:\n",
    "            theta1 = rand_init_weights(Input_layer_size, Hidden_layer_size)\n",
    "            theta2 = rand_init_weights(Hidden_layer_size, Output_layer_size)\n",
    "        else:\n",
    "            theta1 = np.zeros((Hidden_layer_size, Input_layer_size + 1))\n",
    "            theta2 = np.zeros((Output_layer_size, Hidden_layer_size + 1))\n",
    "        comm.Barrier()\n",
    "        if comm.rank == 0:\n",
    "            time_bcast_start = time.time()\n",
    "        comm.Bcast([theta1, MPI.DOUBLE])\n",
    "        comm.Barrier()\n",
    "        comm.Bcast([theta2, MPI.DOUBLE])\n",
    "        if comm.rank == 0:\n",
    "            time_bcast_end = time.time()\n",
    "            print('\\tBcast theta1 and theta2 uses {} secs.'.format(time_bcast_end - time_bcast_start))\n",
    "    else:\n",
    "        theta1 = rand_init_weights(Input_layer_size, Hidden_layer_size)\n",
    "        theta2 = rand_init_weights(Hidden_layer_size, Output_layer_size)\n",
    "\n",
    "    cost = 0.0\n",
    "    for i in range(iteration):\n",
    "        time_iter_start = time.time()\n",
    "\n",
    "        if Distributed is True:\n",
    "            # Scatter training data and labels.\n",
    "            \n",
    "            sliced_inputs = np.asarray(np.split(inputs, comm.size))\n",
    "            sliced_labels = np.asarray(np.split(labels, comm.size))\n",
    "            inputs_buf = np.zeros((int(len(inputs)/comm.size), Input_layer_size))\n",
    "            labels_buf = np.zeros(int(len(labels)/comm.size), dtype='uint8')\n",
    "\n",
    "            comm.Barrier()\n",
    "            if comm.rank == 0:\n",
    "                time_scatter_start = time.time()\n",
    "            comm.Scatter(sliced_inputs, inputs_buf)\n",
    "            if comm.rank == 0:\n",
    "                time_scatter_end = time.time()\n",
    "                print('\\tScatter inputs uses {} secs.'.format(time_scatter_end - time_scatter_start))\n",
    "\n",
    "            comm.Barrier()\n",
    "            if comm.rank == 0:\n",
    "                time_scatter_start = time.time()\n",
    "            comm.Scatter(sliced_labels, labels_buf)\n",
    "            if comm.rank == 0:\n",
    "                time_scatter_end = time.time()\n",
    "                print('\\tScatter labels uses {} secs.'.format(time_scatter_end - time_scatter_start))\n",
    "\n",
    "            # Calculate distributed costs and gradients of this iteration\n",
    "            # by cost function.\n",
    "            comm.Barrier()\n",
    "            cost, (theta1_grad, theta2_grad) = cost_function(theta1, theta2,\n",
    "                Input_layer_size, Hidden_layer_size, Output_layer_size,\n",
    "                inputs_buf, labels_buf, regular=0)\n",
    "\n",
    "            # Gather distributed costs and gradients.\n",
    "            comm.Barrier()\n",
    "            cost_buf = [0] * comm.size\n",
    "            try:\n",
    "                cost_buf = comm.gather(cost)\n",
    "                cost = sum(cost_buf) / len(cost_buf)\n",
    "            except TypeError as e:\n",
    "                print('[{0}] {1}'.format(comm.rank, e))\n",
    "\n",
    "            theta1_grad_buf = np.asarray([np.zeros_like(theta1_grad)] * comm.size)\n",
    "            comm.Barrier()\n",
    "            if comm.rank == 0:\n",
    "                time_gather_start = time.time()\n",
    "            comm.Gather(theta1_grad, theta1_grad_buf)\n",
    "            if comm.rank == 0:\n",
    "                time_gather_end = time.time()\n",
    "                print('\\tGather theta1 uses {} secs.'.format(time_gather_end - time_gather_start))\n",
    "            comm.Barrier()\n",
    "            theta1_grad = functools.reduce(np.add, theta1_grad_buf) / comm.size\n",
    "\n",
    "            theta2_grad_buf = np.asarray([np.zeros_like(theta2_grad)] * comm.size)\n",
    "            comm.Barrier()\n",
    "            if comm.rank == 0:\n",
    "                time_gather_start = time.time()\n",
    "            comm.Gather(theta2_grad, theta2_grad_buf)\n",
    "            if comm.rank == 0:\n",
    "                time_gather_end = time.time()\n",
    "                print('\\tGather theta2 uses {} secs.'.format(time_gather_end - time_gather_start))\n",
    "            comm.Barrier()\n",
    "            theta2_grad = functools.reduce(np.add, theta2_grad_buf) / comm.size\n",
    "        else:\n",
    "            cost, (theta1_grad, theta2_grad) = cost_function(theta1, theta2,\n",
    "                Input_layer_size, Hidden_layer_size, Output_layer_size,\n",
    "                inputs, labels, regular=0)\n",
    "\n",
    "        theta1 -= learningrate * theta1_grad\n",
    "        theta2 -= learningrate * theta2_grad\n",
    "\n",
    "        if Distributed is True:\n",
    "           # Sync-up weights for distributed worknodes.\n",
    "           comm.Bcast([theta1, MPI.DOUBLE])\n",
    "           comm.Bcast([theta2, MPI.DOUBLE])\n",
    "           comm.Barrier()\n",
    "\n",
    "        time_iter_end = time.time()\n",
    "        if comm.rank == 0:\n",
    "            print('Iteration {0} (learning rate {2}, iteration {3}), cost: {1}, time: {4}'.format(\n",
    "                i+1, cost, learningrate, iteration, time_iter_end - time_iter_start)\n",
    "            )\n",
    "    return cost, (theta1, theta2)\n",
    "\n",
    "\n",
    "def train(inputs, labels, learningrate=0.8, iteration=50):\n",
    "    cost, model = gradient_descent(inputs, labels, learningrate, iteration)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, inputs):\n",
    "    theta1, theta2 = model\n",
    "    a1 = np.insert(inputs, 0, 1, axis=1)  # add bias, (5000,401)\n",
    "    a2 = np.dot(a1, theta1.T)  # (5000,401) x (401,25)\n",
    "    a2 = sigmoid(a2)\n",
    "    a2 = np.insert(a2, 0, 1, axis=1)  # add bias, (5000,26)\n",
    "    a3 = np.dot(a2, theta2.T)  # (5000,26) x (26,10)\n",
    "    a3 = sigmoid(a3)  # (5000,10)\n",
    "    return [i.argmax()+1 for i in a3]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if Gpu_mode is True:\n",
    "        print('GPU mode')\n",
    "        Matrix_dot = gpu_matrix_dot()\n",
    "    else:\n",
    "        print('CPU mode')\n",
    "        Matrix_dot = np.dot\n",
    "\n",
    "    if Distributed is True:\n",
    "        print('Parallelism: yes')\n",
    "    else:\n",
    "        print('Parallelism: no')\n",
    "\n",
    "    # Note: There are 10 units which present the digits [1-9, 0]\n",
    "    # (in order) in the output layer.\n",
    "    inputs, labels = load_training_data()\n",
    "\n",
    "    # train the model from scratch and predict based on it\n",
    "    model = train(inputs, labels, learningrate=2, iteration=60)\n",
    "\n",
    "    # Load pretrained weights for debugging precision.\n",
    "    # The precision will be around 97% (0.9756).\n",
    "    #weights = load_weights()\n",
    "    #theta1 = weights[0]  # size: 25 entries, each has 401 numbers\n",
    "    #theta2 = weights[1]  # size: 10 entries, each has  26 numbers\n",
    "    #model = (theta1, theta2)\n",
    "    #cost, (theta1_grad, theta2_grad) = cost_function(theta1, theta2, Input_layer_size, Hidden_layer_size, Output_layer_size, inputs, labels, regular=0)\n",
    "    #print('cost:', cost)\n",
    "\n",
    "    outputs = predict(model, inputs)\n",
    "\n",
    "    correct_prediction = 0\n",
    "    for i, predict in enumerate(outputs):\n",
    "        if predict == labels[i]:\n",
    "            correct_prediction += 1\n",
    "    precision = float(correct_prediction) / len(labels)\n",
    "    print('precision: {}'.format(precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mpiexec -np 4 python mnist-nn-data-parallelism.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cupy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcp\u001b[39;00m  \u001b[38;5;66;03m# Thay np bằng cp để chạy trên GPU\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msio\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cupy'"
     ]
    }
   ],
   "source": [
    "import cupy as cp  # Thay np bằng cp để chạy trên GPU\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Cấu trúc mạng nơ-ron\n",
    "Input_layer_size = 400\n",
    "Hidden_layer_size = 25\n",
    "Output_layer_size = 10\n",
    "\n",
    "# Hàm tiện ích\n",
    "def convert_memory_ordering_f2c(array):\n",
    "    \"\"\"Chuyển ma trận sang định dạng C nếu đang ở định dạng Fortran.\"\"\"\n",
    "    return cp.ascontiguousarray(array) if cp.isfortran(array) else array\n",
    "\n",
    "# Vector hóa sigmoid để tăng hiệu suất trên GPU\n",
    "sigmoid = cp.vectorize(lambda z: 1.0 / (1 + cp.exp(-z)))\n",
    "sigmoid_gradient = cp.vectorize(lambda z: sigmoid(z) * (1 - sigmoid(z)))\n",
    "\n",
    "def load_and_split_data(training_file='mnistdata.mat', test_size=0.2, random_state=42):\n",
    "    \"\"\"Tải và chia dữ liệu MNIST thành tập train/test, chuyển sang GPU.\"\"\"\n",
    "    training_data = sio.loadmat(training_file)\n",
    "    inputs = convert_memory_ordering_f2c(training_data['X'].astype('float32'))\n",
    "    labels = convert_memory_ordering_f2c(training_data['y'].ravel())\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        inputs, labels, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    # Chuyển dữ liệu sang GPU\n",
    "    return (cp.array(X_train), cp.array(X_test), \n",
    "            cp.array(y_train), cp.array(y_test))\n",
    "\n",
    "def rand_init_weights(size_in, size_out):\n",
    "    \"\"\"Khởi tạo weights ngẫu nhiên bằng phương pháp Xavier trên GPU.\"\"\"\n",
    "    epsilon_init = cp.sqrt(6) / cp.sqrt(size_in + size_out)\n",
    "    return cp.random.randn(size_out, size_in + 1) * epsilon_init\n",
    "\n",
    "def cost_function(theta1, theta2, inputs, labels, regular=0.01):\n",
    "    \"\"\"Tính hàm mất mát và gradient trên GPU.\"\"\"\n",
    "    m = len(inputs)\n",
    "    \n",
    "    # Forward propagation\n",
    "    a1 = cp.hstack([cp.ones((m, 1)), inputs])  # (m, 401)\n",
    "    z2 = cp.dot(a1, theta1.T)                  # (m, 25)\n",
    "    a2 = sigmoid(z2)\n",
    "    a2 = cp.hstack([cp.ones((m, 1)), a2])     # (m, 26)\n",
    "    z3 = cp.dot(a2, theta2.T)                  # (m, 10)\n",
    "    h = sigmoid(z3)\n",
    "    \n",
    "    # Chuyển labels thành ma trận one-hot\n",
    "    y_matrix = cp.eye(Output_layer_size)[labels-1]\n",
    "    \n",
    "    # Tính cost với regularization\n",
    "    cost = (-1/m) * cp.sum(y_matrix * cp.log(h) + (1 - y_matrix) * cp.log(1 - h))\n",
    "    reg = (regular/(2*m)) * (cp.sum(cp.square(theta1[:, 1:])) + cp.sum(cp.square(theta2[:, 1:])))\n",
    "    total_cost = cost + reg\n",
    "    \n",
    "    # Backpropagation\n",
    "    delta3 = h - y_matrix                      # (m, 10)\n",
    "    delta2 = cp.dot(delta3, theta2[:, 1:]) * sigmoid_gradient(z2)  # (m, 25)\n",
    "    \n",
    "    theta2_grad = (1/m) * cp.dot(delta3.T, a2) + (regular/m) * cp.hstack([cp.zeros((Output_layer_size, 1)), theta2[:, 1:]])\n",
    "    theta1_grad = (1/m) * cp.dot(delta2.T, a1) + (regular/m) * cp.hstack([cp.zeros((Hidden_layer_size, 1)), theta1[:, 1:]])\n",
    "    \n",
    "    return total_cost, (theta1_grad, theta2_grad)\n",
    "\n",
    "def gradient_descent(inputs, labels, learning_rate=0.8, iterations=50):\n",
    "    \"\"\"Huấn luyện mạng nơ-ron bằng gradient descent trên GPU.\"\"\"\n",
    "    # Khởi tạo weights trên GPU\n",
    "    theta1 = rand_init_weights(Input_layer_size, Hidden_layer_size)\n",
    "    theta2 = rand_init_weights(Hidden_layer_size, Output_layer_size)\n",
    "    \n",
    "    # Gradient descent\n",
    "    for i in range(iterations):\n",
    "        iter_start = time.time()\n",
    "        cost, (theta1_grad, theta2_grad) = cost_function(theta1, theta2, inputs, labels)\n",
    "        theta1 -= learning_rate * theta1_grad\n",
    "        theta2 -= learning_rate * theta2_grad\n",
    "        iter_time = time.time() - iter_start\n",
    "        # Chuyển cost về CPU để in\n",
    "        cost_cpu = cp.asnumpy(cost)\n",
    "        print(f\"Iteration {i+1}/{iterations}, Cost: {cost_cpu:.4f}, Time: {iter_time:.2f}s\")\n",
    "    \n",
    "    return cost, (theta1, theta2)\n",
    "\n",
    "def predict(model, inputs):\n",
    "    \"\"\"Dự đoán nhãn từ inputs trên GPU.\"\"\"\n",
    "    theta1, theta2 = model\n",
    "    a1 = cp.hstack([cp.ones((len(inputs), 1)), inputs])\n",
    "    a2 = sigmoid(cp.dot(a1, theta1.T))\n",
    "    a2 = cp.hstack([cp.ones((len(a2), 1)), a2])\n",
    "    h = sigmoid(cp.dot(a2, theta2.T))\n",
    "    return cp.argmax(h, axis=1) + 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running in GPU mode with CuPy\")\n",
    "    \n",
    "    # Load và chia dữ liệu, chuyển sang GPU\n",
    "    X_train, X_test, y_train, y_test = load_and_split_data()\n",
    "    \n",
    "    # Đo thời gian huấn luyện\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Huấn luyện mô hình trên GPU\n",
    "    cost, model = gradient_descent(X_train, y_train, learning_rate=2, iterations=60)\n",
    "    \n",
    "    # Tính thời gian huấn luyện\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Đánh giá mô hình\n",
    "    train_pred = predict(model, X_train)\n",
    "    train_accuracy = cp.mean(train_pred == y_train)\n",
    "    \n",
    "    test_pred = predict(model, X_test)\n",
    "    test_accuracy = cp.mean(test_pred == y_test)\n",
    "    \n",
    "    # Chuyển kết quả về CPU để in\n",
    "    train_accuracy_cpu = cp.asnumpy(train_accuracy)\n",
    "    test_accuracy_cpu = cp.asnumpy(test_accuracy)\n",
    "    \n",
    "    # In kết quả\n",
    "    print(f\"\\nTraining accuracy: {train_accuracy_cpu*100:.2f}%\")\n",
    "    print(f\"Test accuracy: {test_accuracy_cpu*100:.2f}%\")\n",
    "    print(f\"Total training time: {training_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
